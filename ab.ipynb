{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy.random as rnd\n",
    "\n",
    "def ETF_find(etflistloc, stock):\n",
    "    \"\"\"\n",
    "    reading a file containing information on stock memberships\n",
    "    input: stock ticker\n",
    "    output: corresponding ETF ticker\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(etflistloc)\n",
    "    out = np.array(data['ticker_y'][data['ticker_x']==stock])[0]\n",
    "    return out\n",
    "\n",
    "def excessreturns_closeonly(dataloc, stock, etf, plotcheck = False):\n",
    "    \"\"\"\n",
    "    function to get a time series of DAILY CLOSING\n",
    "    etf-excess log returns for a given stock\n",
    "    all prices are adjusted for stock events\n",
    "    input: location of datasets, stock ticker, etf ticker\n",
    "    output: time series of etf excess log returns\n",
    "    optional: plot sanity check\n",
    "    \"\"\"\n",
    "    s_df = pd.read_csv(dataloc+stock+\".csv\")\n",
    "    e_df = pd.read_csv(dataloc+etf+\".csv\")\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    d1 = pd.to_datetime(\"2022-01-01\")\n",
    "    smp = (dates_dt < d1)\n",
    "    s_df = s_df[smp]\n",
    "    e_df = e_df[smp]\n",
    "    s_log = np.log(s_df['AdjClose'])\n",
    "    e_log = np.log(e_df['AdjClose'])\n",
    "    dates_dt = dates_dt[smp]\n",
    "    s_ret = np.diff(s_log)\n",
    "    e_ret = np.diff(e_log)\n",
    "    excessret = s_ret - e_ret\n",
    "\n",
    "    if plotcheck:\n",
    "        plt.figure(stock+\" price\")\n",
    "        plt.title(stock+\" price\")\n",
    "        plt.plot(dates_dt,s_df['AdjClose'])\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.ylabel(\"price in USD\")\n",
    "        plt.show()\n",
    "        plt.figure(\"Returns \"+stock)\n",
    "        plt.title(\"Returns \"+stock)\n",
    "        plt.plot(dates_dt[1:],s_ret, alpha = 0.7, label = 'stock')\n",
    "        plt.plot(dates_dt[1:],e_ret, alpha = 0.7, label = 'etf')\n",
    "        plt.plot(dates_dt[1:],excessret, alpha = 0.7, label = 'excess return')\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return excessret, dates_dt[1:]\n",
    "\n",
    "\n",
    "def excessreturns(dataloc, stock, etf, plotcheck=False):\n",
    "    \"\"\"\n",
    "    Generates a time series of ETF-excess log returns for a given stock.\n",
    "    The function computes alternating open and close log returns, caps extreme returns,\n",
    "    and optionally plots the data for sanity checks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloc : str\n",
    "        Directory path where the CSV files are located.\n",
    "    stock : str\n",
    "        Ticker symbol of the stock.\n",
    "    etf : str\n",
    "        Ticker symbol of the corresponding ETF.\n",
    "    plotcheck : bool, optional\n",
    "        If True, generates plots for the stock price and returns (default is False).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    excessret : np.ndarray\n",
    "        Array of ETF-excess log returns.\n",
    "    dates_dt : pd.DatetimeIndex\n",
    "        Corresponding dates for the returns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the cutoff date\n",
    "    cutoff_date = pd.Timestamp(\"2022-01-01\")\n",
    "\n",
    "    # Read CSV files with date parsing for efficiency\n",
    "    s_df = pd.read_csv(f\"{dataloc}{stock}.csv\", parse_dates=['date'])\n",
    "    e_df = pd.read_csv(f\"{dataloc}{etf}.csv\", parse_dates=['date'])\n",
    "\n",
    "    # Filter data before the cutoff date and reset indices\n",
    "    mask = s_df['date'] < cutoff_date\n",
    "    s_df = s_df.loc[mask].reset_index(drop=True)\n",
    "    e_df = e_df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    # Ensure both DataFrames have the same length after filtering\n",
    "    if len(s_df) != len(e_df):\n",
    "        raise ValueError(f\"Mismatch in data lengths after filtering for {stock} and {etf}.\")\n",
    "\n",
    "    # Compute log of Adjusted Close and Adjusted Open prices\n",
    "    s_logclose = np.log(s_df['AdjClose'].values)\n",
    "    e_logclose = np.log(e_df['AdjClose'].values)\n",
    "    s_logopen = np.log(s_df['AdjOpen'].values)\n",
    "    e_logopen = np.log(e_df['AdjOpen'].values)\n",
    "\n",
    "    # Interleave open and close log prices using vectorized operations\n",
    "    s_log = np.empty(2 * len(s_logclose))\n",
    "    e_log = np.empty(2 * len(e_logclose))\n",
    "    s_log[0::2] = s_logopen\n",
    "    s_log[1::2] = s_logclose\n",
    "    e_log[0::2] = e_logopen\n",
    "    e_log[1::2] = e_logclose\n",
    "\n",
    "    # Calculate log returns\n",
    "    s_ret = np.diff(s_log)\n",
    "    e_ret = np.diff(e_log)\n",
    "\n",
    "    # Cap returns to mitigate the effect of outliers\n",
    "    cap_value = 0.15\n",
    "    s_ret = np.clip(s_ret, -cap_value, cap_value)\n",
    "    e_ret = np.clip(e_ret, -cap_value, cap_value)\n",
    "\n",
    "    # Calculate ETF-excess returns\n",
    "    excessret = s_ret - e_ret\n",
    "\n",
    "    # Align dates: since returns are based on differences, exclude the first date\n",
    "    dates_dt = s_df['date'].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    if plotcheck:\n",
    "        # Plot Adjusted Close Price\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(s_df['date'], s_df['AdjClose'], label=f'{stock} AdjClose', color='blue')\n",
    "        plt.title(f'{stock} Adjusted Close Price')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Returns\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(dates_dt, s_ret, alpha=0.7, label='Stock Returns', color='green')\n",
    "        plt.plot(dates_dt, e_ret, alpha=0.7, label='ETF Returns', color='orange')\n",
    "        plt.plot(dates_dt, excessret, alpha=0.7, label='Excess Returns', color='red')\n",
    "        plt.title(f'Returns for {stock} vs {etf}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Log Return')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return excessret, dates_dt\n",
    "\n",
    "def rawreturns(dataloc, stock, plotcheck = False):\n",
    "    \"\"\"\n",
    "    function to get a time series of raw log returns for a given stock/etf\n",
    "    all prices are adjusted for stock events\n",
    "    input: location of datasets, stock ticker, etf ticker\n",
    "    output: time series of etf excess log returns\n",
    "    optional: plot sanity check\n",
    "    \"\"\"\n",
    "    s_df = pd.read_csv(dataloc+stock+\".csv\")\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    d1 = pd.to_datetime(\"2022-01-01\")\n",
    "    smp = (dates_dt < d1)\n",
    "    s_df = s_df[smp]\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    s_logclose = np.log(s_df['AdjClose'])\n",
    "    s_logopen = np.log(s_df['AdjOpen'])\n",
    "    s_log = np.zeros(2*len(s_logclose))\n",
    "    for i in range(len(s_logclose)):\n",
    "        s_log[2 * i] = s_logopen[i]\n",
    "        s_log[2 * i + 1] = s_logclose[i]\n",
    "    s_ret = np.diff(s_log)\n",
    "    s_ret[s_ret > 0.15] = 0.15\n",
    "    s_ret[s_ret < -0.15] = -0.15\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    if plotcheck:\n",
    "        plt.figure(stock+\" price\")\n",
    "        plt.title(stock+\" price\")\n",
    "        plt.plot(dates_dt,s_df['AdjClose'])\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.ylabel(\"price in USD\")\n",
    "        plt.show()\n",
    "        plt.figure(\"Returns \"+stock)\n",
    "        plt.title(\"Returns \"+stock)\n",
    "        plt.plot(range(len(s_ret)),s_ret)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return s_ret, dates_dt\n",
    "\n",
    "def split_train_val_test(stock, dataloc, etflistloc, tr = 0.8, vl = 0.1, h = 1, l = 10, pred = 1, plotcheck=False):\n",
    "    \"\"\"\n",
    "    prepare etf excess log returns for a given stock\n",
    "    split into train, val, test\n",
    "    h: sliding window\n",
    "    l: condition window (number of previous values)\n",
    "    pred: prediction window\n",
    "    \"\"\"\n",
    "    etf = ETF_find(etflistloc, stock)\n",
    "    excess_returns, dates_dt = excessreturns(dataloc, stock, etf, plotcheck)\n",
    "    N = len(excess_returns)\n",
    "    N_tr = int(tr*N)\n",
    "    N_vl = int(vl*N)\n",
    "    N_tst = N - N_tr - N_vl\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    test_sr = excess_returns[N_tr+N_vl:]\n",
    "    n = int((N_tr-l-pred)/h)+1\n",
    "    train_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        train_data[i,:] = train_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_vl-l-pred)/h)+1\n",
    "    val_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        val_data[i,:] = val_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_tst-l-pred)/h)+1\n",
    "    test_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        test_data[i,:] = test_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    if plotcheck:\n",
    "        plt.figure(\"Excess returns\")\n",
    "        plt.plot(dates_dt,excess_returns)\n",
    "        plt.title(stock+ \" excess returns\")\n",
    "        plt.axvline(x = dates_dt[N_tr],color = \"red\")\n",
    "        plt.axvline(x = dates_dt[N_tr+N_vl],color = \"red\")\n",
    "        plt.show()\n",
    "    return train_data,val_data,test_data, dates_dt\n",
    "\n",
    "def split_train_testraw(stock, dataloc, tr = 0.8, vl = 0.1, h = 1, l = 10, pred = 1, plotcheck=False):\n",
    "    \"\"\"\n",
    "    prepare raw log returns for a given stock\n",
    "    split into train, test\n",
    "    h: sliding window\n",
    "    l: condition window (number of previous values)\n",
    "    pred: prediction window\n",
    "    \"\"\"\n",
    "    excess_returns, dates_dt = rawreturns(dataloc, stock, plotcheck)\n",
    "    N = len(excess_returns)\n",
    "    N_tr = int(tr*N) + int(vl*N)\n",
    "    N_tst = N - N_tr\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    test_sr = excess_returns[N_tr:]\n",
    "    n = int((N_tr-l-pred)/h)+1\n",
    "    train_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        train_data[i,:] = train_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_tst-l-pred)/h)+1\n",
    "    test_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        test_data[i,:] = test_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "\n",
    "    return train_data,test_data\n",
    "\n",
    "def split_train_val_testraw(stock, dataloc, tr = 0.8, vl = 0.1, h = 1, l = 10, pred = 1, plotcheck=False):\n",
    "    \"\"\"\n",
    "    prepare raw log returns for a given stock\n",
    "    split into train, val, test\n",
    "    h: sliding window\n",
    "    l: condition window (number of previous values)\n",
    "    pred: prediction window\n",
    "    \"\"\"\n",
    "    excess_returns, dates_dt = rawreturns(dataloc, stock, plotcheck)\n",
    "    N = len(excess_returns)\n",
    "    N_tr = int(tr*N)\n",
    "    N_vl = int(vl*N)\n",
    "    N_tst = N - N_tr - N_vl\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    test_sr = excess_returns[N_tr+N_vl:]\n",
    "    n = int((N_tr-l-pred)/h)+1\n",
    "    train_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        train_data[i,:] = train_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_vl-l-pred)/h)+1\n",
    "    val_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        val_data[i,:] = val_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_tst-l-pred)/h)+1\n",
    "    test_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        test_data[i,:] = test_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    if plotcheck:\n",
    "        plt.figure(\"returns\")\n",
    "        plt.plot(dates_dt,excess_returns)\n",
    "        plt.title(stock+ \" =returns\")\n",
    "        plt.axvline(x = dates_dt[N_tr],color = \"red\")\n",
    "        plt.axvline(x = dates_dt[N_tr+N_vl],color = \"red\")\n",
    "        plt.show()\n",
    "    return train_data,val_data,test_data, dates_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy.random as rnd\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        noise_dim: the dimension of the noise, a scalar\n",
    "        cond_dim: the dimension of the condition, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, noise_dim,cond_dim, hidden_dim,output_dim,mean,std):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = noise_dim+cond_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        #predicting a single value, so the output dimension is 1\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        #Add the modules\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=cond_dim, hidden_size=self.hidden_dim, num_layers=1, dropout=0)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        self.linear1 = nn.Linear(in_features=self.hidden_dim+self.noise_dim, out_features=self.hidden_dim+self.noise_dim)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        self.linear2 = nn.Linear(in_features=self.hidden_dim+self.noise_dim, out_features=output_dim)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, noise,condition,h_0,c_0):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator:adding the noise and the condition separately\n",
    "        '''\n",
    "        #x = combine_vectors(noise.to(torch.float),condition.to(torch.float),2)\n",
    "        condition = (condition-self.mean)/self.std\n",
    "        out, (h_n, c_n) = self.lstm(condition, (h_0, c_0))\n",
    "        out = combine_vectors(noise.to(torch.float),h_n.to(torch.float),dim=-1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.linear2(out)\n",
    "        out = out*self.std+self.mean\n",
    "        return out\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    '''\n",
    "    Values:\n",
    "        cond_dim: the dimension of the condition, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, noise_dim,cond_dim, hidden_dim,output_dim,mean,std):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = noise_dim+cond_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        #predicting a single value, so the output dimension is 1\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        #Add the modules\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=cond_dim, hidden_size=self.output_dim, num_layers=1, dropout=0)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, condition,h_0,c_0):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator:adding the noise and the condition separately\n",
    "        '''\n",
    "        #x = combine_vectors(noise.to(torch.float),condition.to(torch.float),2)\n",
    "        condition = (condition-self.mean)/self.std\n",
    "        out, (h_n, c_n) = self.lstm(condition, (h_0, c_0))\n",
    "        out = out*self.std+self.mean\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "      in_dim: the input dimension (noise dim + conditin dim + forecast dim for the condition for this dataset), a scalar\n",
    "      hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim,mean,std):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=self.hidden_dim, num_layers=1, dropout=0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        self.linear = nn.Linear(in_features=self.hidden_dim, out_features=1)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, in_chan,h_0,c_0):\n",
    "        '''\n",
    "        in_chan: concatenated condition with real or fake\n",
    "        h_0 and c_0: for the LSTM\n",
    "        '''\n",
    "        x = in_chan\n",
    "        x = (x-self.mean)/self.std\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        out = self.linear(h_n)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy.random as rnd\n",
    "\n",
    "def combine_vectors(x, y,dim=-1):\n",
    "    '''\n",
    "    Function for combining two tensors\n",
    "    '''\n",
    "    combined = torch.cat([x,y],dim=dim)\n",
    "    combined = combined.to(torch.float)\n",
    "    return combined\n",
    "\n",
    "def getPnL(predicted,real,nsamp):\n",
    "    \"\"\"\n",
    "    PnL per trade given nsamp samples, predicted forecast, real data realisations\n",
    "    in bpts\n",
    "    \"\"\"\n",
    "    sgn_fake = torch.sign(predicted)\n",
    "    PnL = torch.sum(sgn_fake*real)\n",
    "    PnL = 10000*PnL/nsamp\n",
    "    return PnL\n",
    "\n",
    "def getSR(predicted,real):\n",
    "    \"\"\"\n",
    "    Sharpe Ratio given forecasts predicted of real (not annualised)\n",
    "    \"\"\"\n",
    "    sgn_fake = torch.sign(predicted)\n",
    "    SR = torch.mean(sgn_fake * real) / torch.std(sgn_fake * real)\n",
    "    return SR\n",
    "\n",
    "def Evaluation2(ticker,freq,gen,test_data, val_data, h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, losstype, sr_val, device, plotsloc, f_name, plot = False):\n",
    "    \"\"\"\n",
    "    Evaluation of a GAN model on a single stock\n",
    "    \"\"\"\n",
    "    df_temp = False\n",
    "    dt = {'lrd':lrd,'lrg':lrg,'type': losstype,'epochs':n_epochs, 'ticker':ticker,  'hid_g':hid_g, 'hid_d':hid_d}\n",
    "    #print(\"Validation set best PnL (in bp): \",PnL_best)\n",
    "    #print(\"Checkpoint epoch: \",checkpoint_last_epoch+1)\n",
    "    ntest = test_data.shape[0]\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        condition1 = test_data[:,0:l]\n",
    "        condition1 = condition1.unsqueeze(0)\n",
    "        condition1 = condition1.to(device)\n",
    "        condition1 = condition1.to(torch.float)\n",
    "        ntest = test_data.shape[0]\n",
    "        h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "        fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "        fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "        generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "        generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "        #generated1 = fake1.detach()\n",
    "        for i in range(999):\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            #print(fake.shape)\n",
    "            generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "#             print(generated1.shape)\n",
    "            del fake1\n",
    "            del fake_noise\n",
    "        #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "        #mae = torch.mean(torch.abs(fake-real))\n",
    "    #print(\"RMSE: \", rmse)\n",
    "    #print(\"MAE: \",mae)\n",
    "    b1 = generated1.squeeze()\n",
    "    mn1 = torch.mean(b1,dim=1)\n",
    "    real1 = test_data[:,-1]\n",
    "    rl1 = real1.squeeze()\n",
    "    rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "    mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "    #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "    dt['RMSE'] = rmse1.item()\n",
    "    dt['MAE'] = mae1.item()\n",
    "    ft1 = mn1.clone().detach().to(device)\n",
    "    PnL1 = getPnL(ft1,rl1,ntest)\n",
    "    #print(\"PnL in bp\", PnL)\n",
    "\n",
    "    #look at the Sharpe Ratio\n",
    "    n_b1 = b1.shape[1]\n",
    "    PnL_ws1 = torch.empty(ntest)\n",
    "    for i1 in range(ntest):\n",
    "        fk1 = b1[i1,:]\n",
    "        pu1 = (fk1>=0).sum()\n",
    "        pu1 = pu1/n_b1\n",
    "        pd1 = 1-pu1\n",
    "        PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "        PnL_ws1[i1] = PnL_temp1.item()\n",
    "    PnL_ws1 = np.array(PnL_ws1)\n",
    "    PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    PnL_even = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    PnL_odd = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    for i1 in range(len(PnL_wd1)):\n",
    "        PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "        PnL_even[i1] = PnL_ws1[2 * i1]\n",
    "        PnL_odd[i1] = PnL_ws1[2 * i1 + 1]\n",
    "    PnL_test = PnL_wd1\n",
    "    PnL_w_m1 = np.mean(PnL_wd1)\n",
    "    PnL_w_std1 = np.std(PnL_wd1)\n",
    "    SR1 = PnL_w_m1/PnL_w_std1\n",
    "    #print(\"Sharpe Ratio: \",SR)\n",
    "    dt['SR_w scaled'] = SR1*np.sqrt(252)\n",
    "    dt['PnL_w'] = PnL_w_m1\n",
    "\n",
    "    if (ntest % 2) == 0:\n",
    "        dt['Close-to-Open SR_w'] = np.sqrt(252) * np.mean(PnL_even) / np.std(PnL_even)\n",
    "        dt['Open-to-Close SR_w'] = np.sqrt(252) * np.mean(PnL_odd) / np.std(PnL_odd)\n",
    "    else:\n",
    "        dt['Open-to-Close SR_w'] = np.sqrt(252) * np.mean(PnL_even) / np.std(PnL_even)\n",
    "        dt['Close-to-Open SR_w'] = np.sqrt(252) * np.mean(PnL_odd) / np.std(PnL_odd)\n",
    "    print(\"Annualised (test) SR_w: \",SR1*np.sqrt(252))\n",
    "\n",
    "    distcheck = np.array(b1[1,:].cpu())\n",
    "    means = np.array(mn1.detach())\n",
    "    reals = np.array(rl1.detach())\n",
    "    dt['Corr'] = np.corrcoef([means,reals])[0,1]\n",
    "    dt['Pos mn'] = np.sum(means >0)/ len(means)\n",
    "    dt['Neg mn'] = np.sum(means <0)/ len(means)\n",
    "    print('Correlation ',np.corrcoef([means,reals])[0,1] )\n",
    "\n",
    "    dt['narrow dist'] = (np.std(distcheck)<0.0002)\n",
    "\n",
    "    means_gen = means\n",
    "    reals_test = reals\n",
    "    distcheck_test = distcheck\n",
    "    rl_test = reals[1]\n",
    "\n",
    "    mn = torch.mean(b1,dim=1)\n",
    "    mn = np.array(mn.cpu())\n",
    "    dt['narrow means dist'] = (np.std(mn)<0.0002)\n",
    "\n",
    "    ntest = val_data.shape[0]\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        condition1 = val_data[:,0:l]\n",
    "        condition1 = condition1.unsqueeze(0)\n",
    "        condition1 = condition1.to(device)\n",
    "        condition1 = condition1.to(torch.float)\n",
    "        ntest = val_data.shape[0]\n",
    "        h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "        fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "        fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "        generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "        generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "        #generated1 = fake1.detach()\n",
    "        for i in range(999):\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            #print(fake.shape)\n",
    "            generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "#             print(generated1.shape)\n",
    "            del fake1\n",
    "            del fake_noise\n",
    "        #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "        #mae = torch.mean(torch.abs(fake-real))\n",
    "    #print(\"RMSE: \", rmse)\n",
    "    #print(\"MAE: \",mae)\n",
    "    b1 = generated1.squeeze()\n",
    "    mn1 = torch.mean(b1,dim=1)\n",
    "    real1 = val_data[:,-1]\n",
    "    rl1 = real1.squeeze()\n",
    "    rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "    mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "    #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "    dt['RMSE val'] = rmse1.item()\n",
    "    dt['MAE val'] = mae1.item()\n",
    "    ft1 = mn1.clone().detach().to(device)\n",
    "    #print(\"PnL in bp\", PnL)\n",
    "\n",
    "    #look at the Sharpe Ratio\n",
    "    n_b1 = b1.shape[1]\n",
    "    PnL_ws1 = torch.empty(ntest)\n",
    "    for i1 in range(ntest):\n",
    "        fk1 = b1[i1,:]\n",
    "        pu1 = (fk1>=0).sum()\n",
    "        pu1 = pu1/n_b1\n",
    "        pd1 = 1-pu1\n",
    "        PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "        PnL_ws1[i1] = PnL_temp1.item()\n",
    "    PnL_ws1 = np.array(PnL_ws1)\n",
    "    PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    for i1 in range(len(PnL_wd1)):\n",
    "        PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "    PnL_w_m1 = np.mean(PnL_wd1)\n",
    "    PnL_w_std1 = np.std(PnL_wd1)\n",
    "    SR1 = PnL_w_m1/PnL_w_std1\n",
    "    #print(\"Sharpe Ratio: \",SR)\n",
    "    dt['PnL_w val'] = PnL_w_m1\n",
    "    dt['SR_w scaled val'] = SR1*np.sqrt(252)\n",
    "\n",
    "    print(\"Annualised (val) SR_w : \",SR1*np.sqrt(252))\n",
    "\n",
    "    means = np.array(mn1.detach())\n",
    "    reals = np.array(rl1.detach())\n",
    "    dt['Corr val'] = np.corrcoef([means,reals])[0,1]\n",
    "    dt['Pos mn val'] = np.sum(means >0)/ len(means)\n",
    "    dt['Neg mn val'] = np.sum(means <0)/ len(means)\n",
    "\n",
    "    df_temp = pd.DataFrame(data=dt,index=[0])\n",
    "\n",
    "    return df_temp, PnL_test, PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test\n",
    "\n",
    "def Evaluation3(tickers,freq,gen,test, val, h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, losstype, sr_val, device, plotsloc, f_name, plot = False):\n",
    "    \"\"\"\n",
    "    Evaluation of a GAN model in the universality setting (multiple tickers)\n",
    "    \"\"\"\n",
    "    df_temp = False\n",
    "    dt = {'lrd':[],'lrg':[],'type': [],'epochs':[], 'ticker':[],  'hid_g':[], 'hid_d':[]}\n",
    "    results_df = pd.DataFrame(data = dt)\n",
    "    PnLs_test = np.zeros((len(tickers), int(0.5 * test[0].shape[0])))\n",
    "    PnLs_val = np.zeros((len(tickers), int(0.5 * val[0].shape[0])))\n",
    "    means_test = np.zeros((len(tickers), test[0].shape[0]))\n",
    "    means_val = np.zeros((len(tickers), val[0].shape[0]))\n",
    "    # print(means_test.shape)\n",
    "    #print(\"Validation set best PnL (in bp): \",PnL_best)\n",
    "    #print(\"Checkpoint epoch: \",checkpoint_last_epoch+1)\n",
    "    for ii in tqdm(range(len(tickers))):\n",
    "        val_data = val[ii]\n",
    "        test_data = test[ii]\n",
    "        ticker = tickers[ii]\n",
    "        dt = {'lrd':lrd,'lrg':lrg,'type': losstype,'epochs':n_epochs, 'ticker':ticker,  'hid_g':hid_g, 'hid_d':hid_d}\n",
    "        ntest = test_data.shape[0]\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            condition1 = test_data[:,0:l]\n",
    "            condition1 = condition1.unsqueeze(0)\n",
    "            condition1 = condition1.to(device)\n",
    "            condition1 = condition1.to(torch.float)\n",
    "            ntest = test_data.shape[0]\n",
    "            h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "            generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = fake1.detach()\n",
    "            for i in range(999):\n",
    "                fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "                fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "                fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "                #print(fake.shape)\n",
    "                generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "                #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "    #             print(generated1.shape)\n",
    "                del fake1\n",
    "                del fake_noise\n",
    "            #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "            #mae = torch.mean(torch.abs(fake-real))\n",
    "        #print(\"RMSE: \", rmse)\n",
    "        #print(\"MAE: \",mae)\n",
    "        b1 = generated1.squeeze()\n",
    "        mn1 = torch.mean(b1,dim=1)\n",
    "        # print(mn1.shape)\n",
    "        means_test[ii, :] = np.array(mn1.detach())\n",
    "        real1 = test_data[:,-1]\n",
    "        rl1 = real1.squeeze()\n",
    "        rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "        mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "        #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "        dt['RMSE'] = rmse1.item()\n",
    "        dt['MAE'] = mae1.item()\n",
    "        ft1 = mn1.clone().detach().to(device)        #print(\"PnL in bp\", PnL)\n",
    "\n",
    "        #look at the Sharpe Ratio\n",
    "        n_b1 = b1.shape[1]\n",
    "        PnL_ws1 = torch.empty(ntest)\n",
    "        for i1 in range(ntest):\n",
    "            fk1 = b1[i1,:]\n",
    "            pu1 = (fk1>=0).sum()\n",
    "            pu1 = pu1/n_b1\n",
    "            pd1 = 1-pu1\n",
    "            PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "            PnL_ws1[i1] = PnL_temp1.item()\n",
    "        PnL_ws1 = np.array(PnL_ws1)\n",
    "        PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "        for i1 in range(len(PnL_wd1)):\n",
    "            PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "        PnLs_test[ii, :] = PnL_wd1\n",
    "        PnL_w_m1 = np.mean(PnL_wd1)\n",
    "        PnL_w_std1 = np.std(PnL_wd1)\n",
    "        SR1 = PnL_w_m1/PnL_w_std1\n",
    "        #print(\"Sharpe Ratio: \",SR)\n",
    "        dt['PnL_w'] = PnL_w_m1\n",
    "        dt['SR_w scaled'] = SR1 * np.sqrt(252)\n",
    "        # print(\"Annualised (test) SR_w: \",SR1*np.sqrt(252 * freq))\n",
    "        # print(\"Annualised (test) SR_m: \", np.sqrt(252 * freq) * getSR(ft1,rl1).item())\n",
    "        dist_loc = plotsloc+\"distcheck-\"+f_name+\".png\"\n",
    "\n",
    "        distcheck = np.array(b1[1,:].cpu())\n",
    "        means = np.array(mn1.detach())\n",
    "        reals = np.array(rl1.detach())\n",
    "        dt['Corr'] = np.corrcoef([means,reals])[0,1]\n",
    "        dt['Pos mn'] = np.sum(means >0)/ len(means)\n",
    "        dt['Neg mn'] = np.sum(means <0)/ len(means)\n",
    "        # print('Correlation ',np.corrcoef([means,reals])[0,1] )\n",
    "\n",
    "        dt['narrow dist'] = (np.std(distcheck)<0.0002)\n",
    "\n",
    "        means_loc = plotsloc+\"recovered-means-\"+f_name+\".png\"\n",
    "\n",
    "\n",
    "        mn = torch.mean(b1,dim=1)\n",
    "        mn = np.array(mn.cpu())\n",
    "        dt['narrow means dist'] = (np.std(mn)<0.0002)\n",
    "\n",
    "\n",
    "        ntest = val_data.shape[0]\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            condition1 = val_data[:,0:l]\n",
    "            condition1 = condition1.unsqueeze(0)\n",
    "            condition1 = condition1.to(device)\n",
    "            condition1 = condition1.to(torch.float)\n",
    "            ntest = val_data.shape[0]\n",
    "            h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "            generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = fake1.detach()\n",
    "            for i in range(999):\n",
    "                fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "                fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "                fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "                #print(fake.shape)\n",
    "                generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "                #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "    #             print(generated1.shape)\n",
    "                del fake1\n",
    "                del fake_noise\n",
    "            #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "            #mae = torch.mean(torch.abs(fake-real))\n",
    "        #print(\"RMSE: \", rmse)\n",
    "        #print(\"MAE: \",mae)\n",
    "        b1 = generated1.squeeze()\n",
    "        mn1 = torch.mean(b1,dim=1)\n",
    "        means_val[ii, :] = np.array(mn1.detach())\n",
    "\n",
    "        real1 = val_data[:,-1]\n",
    "        rl1 = real1.squeeze()\n",
    "        rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "        mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "        #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "        dt['RMSE val'] = rmse1.item()\n",
    "        dt['MAE val'] = mae1.item()\n",
    "        ft1 = mn1.clone().detach().to(device)\n",
    "        #print(\"PnL in bp\", PnL)\n",
    "\n",
    "        #look at the Sharpe Ratio\n",
    "        n_b1 = b1.shape[1]\n",
    "        PnL_ws1 = torch.empty(ntest)\n",
    "        for i1 in range(ntest):\n",
    "            fk1 = b1[i1,:]\n",
    "            pu1 = (fk1>=0).sum()\n",
    "            pu1 = pu1/n_b1\n",
    "            pd1 = 1-pu1\n",
    "            PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "            PnL_ws1[i1] = PnL_temp1.item()\n",
    "        PnL_ws1 = np.array(PnL_ws1)\n",
    "\n",
    "        PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "        for i1 in range(len(PnL_wd1)):\n",
    "            PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "        PnLs_val[ii, :] = PnL_wd1\n",
    "        PnL_w_m1 = np.mean(PnL_wd1)\n",
    "        PnL_w_std1 = np.std(PnL_wd1)\n",
    "        SR1 = PnL_w_m1/PnL_w_std1\n",
    "        #print(\"Sharpe Ratio: \",SR)\n",
    "        dt['PnL_w val'] = PnL_w_m1\n",
    "        dt['SR_w scaled val'] = SR1*np.sqrt(freq)\n",
    "\n",
    "\n",
    "        # print(\"Annualised (val) SR_w : \",SR1*np.sqrt(252 * freq))\n",
    "        # print(\"Annualised (val) SR_m : \", np.sqrt(252 * freq) * getSR(ft1,rl1).item())\n",
    "\n",
    "        means = np.array(mn1.detach())\n",
    "        reals = np.array(rl1.detach())\n",
    "        dt['Corr val'] = np.corrcoef([means,reals])[0,1]\n",
    "        dt['Pos mn val'] = np.sum(means >0)/ len(means)\n",
    "        dt['Neg mn val'] = np.sum(means <0)/ len(means)\n",
    "        df_temp = pd.DataFrame(data=dt,index=[0])\n",
    "        results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    PnL_test = np.sum(PnLs_test,axis=0)\n",
    "    PnL_val = np.sum(PnLs_val,axis=0)\n",
    "\n",
    "    return results_df, PnL_test, PnL_val, means_test, means_val\n",
    "\n",
    "def GradientCheck(ticker, gen, disc, gen_opt, disc_opt, criterion, n_epochs, train_data,batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Gradient norm check\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    BCE_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    PnL_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    MSE_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    SR_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    STD_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "    #currstep = 0\n",
    "    #train the discriminator more\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            MSE = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "            STD = torch.std(PnL_s)\n",
    "            gen_opt.zero_grad()\n",
    "            SR.backward(retain_graph=True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "            #list of gradient norms\n",
    "            SR_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            PnL.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            PnL_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            MSE.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            MSE_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            STD.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            STD_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            gen_loss.backward()\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            BCE_norm[epoch*nbatches+i] = total_norm\n",
    "            gen_opt.step()\n",
    "\n",
    "\n",
    "    alpha = torch.mean(BCE_norm / PnL_norm)\n",
    "    beta =  torch.mean(BCE_norm / MSE_norm)\n",
    "    gamma =  torch.mean(BCE_norm / SR_norm)\n",
    "    delta = torch.mean(BCE_norm / STD_norm)\n",
    "    print(\"Completed. \")\n",
    "    print(r\"$\\alpha$:\", alpha)\n",
    "    print(r\"$\\beta$:\", beta)\n",
    "    print(r\"$\\gamma$:\", gamma)\n",
    "    print(r\"$\\delta$:\", delta)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(ticker + \" BCE norm\")\n",
    "        plt.title(ticker + \" BCE norm\")\n",
    "        plt.plot(range(len(BCE_norm)),BCE_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" PnL norm\")\n",
    "        plt.title(ticker +\" PnL norm\")\n",
    "        plt.plot(range(len(BCE_norm)),PnL_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" MSE norm\")\n",
    "        plt.title(ticker + \" MSE norm\")\n",
    "        plt.plot(range(len(BCE_norm)), MSE_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" SR norm\")\n",
    "        plt.title(\"SR norm\")\n",
    "        plt.plot(range(len(BCE_norm)),SR_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" STD norm\")\n",
    "        plt.title(ticker + \" STD norm\")\n",
    "        plt.plot(range(len(BCE_norm)),STD_norm)\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.show()\n",
    "\n",
    "        # plt.figure(ticker + \" Norms\")\n",
    "        # plt.title(ticker + \" gradient norms\")\n",
    "        # plt.plot(range(len(BCE_norm)),BCE_norm, label = \"BCE\")\n",
    "        # plt.plot(range(len(BCE_norm)),PnL_norm, label = \"PnL\")\n",
    "        # plt.plot(range(len(BCE_norm)),SR_norm, label = \"SR\")\n",
    "        # plt.plot(range(len(BCE_norm)),STD_norm, label = \"STD\")\n",
    "        # plt.ylabel(r\"$L^2$ norm\")\n",
    "        # plt.xlabel(\"iteration\")\n",
    "        # plt.legend(loc = 'best')\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "    return gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy.random as rnd\n",
    "\n",
    "def TrainLoopMainPnLnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot=False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL and MSE loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "    SR_best = 0\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + beta * SqLoss\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed \")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLMSESRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL, MSE, SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + beta * SqLoss - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLMSESTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL, MSE, STD loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + beta * SqLoss + delta * torch.std(PnL_s)\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "    SR_best = 0\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: MSE loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))  + beta * SqLoss\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainSRMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: SR, MSE loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) + beta * SqLoss - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLSTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL, STD loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "    SR_best = 0\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            STD = torch.std(PnL_s)\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + delta * STD\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    return gen, disc, gen_opt, disc_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy.random as rnd\n",
    "\n",
    "def TrainLoopForGAN(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for the BCE GAN (ForGAN)\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed, checkpoint epoch: \", checkpoint_last_epoch)\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Oct 26 2023\n",
    "\n",
    "@author: vuletic@maths.ox.ac.uk\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy.random as rnd\n",
    "\n",
    "def ETF_find(etflistloc, stock):\n",
    "    \"\"\"\n",
    "    reading a file containing information on stock memberships\n",
    "    input: stock ticker\n",
    "    output: corresponding ETF ticker\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(etflistloc)\n",
    "    out = np.array(data['ticker_y'][data['ticker_x']==stock])[0]\n",
    "    return out\n",
    "\n",
    "\n",
    "def excessreturns_closeonly(dataloc, stock, etf, plotcheck = False):\n",
    "    \"\"\"\n",
    "    function to get a time series of DAILY CLOSING\n",
    "    etf-excess log returns for a given stock\n",
    "    all prices are adjusted for stock events\n",
    "    input: location of datasets, stock ticker, etf ticker\n",
    "    output: time series of etf excess log returns\n",
    "    optional: plot sanity check\n",
    "    \"\"\"\n",
    "    s_df = pd.read_csv(dataloc+stock+\".csv\")\n",
    "    e_df = pd.read_csv(dataloc+etf+\".csv\")\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    d1 = pd.to_datetime(\"2022-01-01\")\n",
    "    smp = (dates_dt < d1)\n",
    "    s_df = s_df[smp]\n",
    "    e_df = e_df[smp]\n",
    "    s_log = np.log(s_df['AdjClose'])\n",
    "    e_log = np.log(e_df['AdjClose'])\n",
    "    dates_dt = dates_dt[smp]\n",
    "    s_ret = np.diff(s_log)\n",
    "    e_ret = np.diff(e_log)\n",
    "    excessret = s_ret - e_ret\n",
    "\n",
    "    if plotcheck:\n",
    "        plt.figure(stock+\" price\")\n",
    "        plt.title(stock+\" price\")\n",
    "        plt.plot(dates_dt,s_df['AdjClose'])\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.ylabel(\"price in USD\")\n",
    "        plt.show()\n",
    "        plt.figure(\"Returns \"+stock)\n",
    "        plt.title(\"Returns \"+stock)\n",
    "        plt.plot(dates_dt[1:],s_ret, alpha = 0.7, label = 'stock')\n",
    "        plt.plot(dates_dt[1:],e_ret, alpha = 0.7, label = 'etf')\n",
    "        plt.plot(dates_dt[1:],excessret, alpha = 0.7, label = 'excess return')\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return excessret, dates_dt[1:]\n",
    "\n",
    "def excessreturns(dataloc, stock, etf, plotcheck=False):\n",
    "    \"\"\"\n",
    "    Generates a time series of ETF-excess log returns for a given stock.\n",
    "    The function computes alternating open and close log returns, caps extreme returns,\n",
    "    and optionally plots the data for sanity checks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloc : str\n",
    "        Directory path where the CSV files are located. Ensure it ends with a '/'.\n",
    "    stock : str\n",
    "        Ticker symbol of the stock (e.g., 'TCS').\n",
    "    etf : str\n",
    "        Ticker symbol of the corresponding ETF (e.g., '^CNXIT').\n",
    "    plotcheck : bool, optional\n",
    "        If True, generates plots for the stock price and returns (default is False).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    excessret : np.ndarray\n",
    "        Array of ETF-excess log returns.\n",
    "    dates_dt : pd.DatetimeIndex\n",
    "        Corresponding dates for the returns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the cutoff date\n",
    "    cutoff_date = pd.Timestamp(\"2022-01-01\")\n",
    "\n",
    "    # Read CSV files with date parsing for efficiency\n",
    "    try:\n",
    "        s_df = pd.read_csv(f\"{dataloc}{stock}.csv\", parse_dates=['date'])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Stock file '{dataloc}{stock}.csv' not found.\")\n",
    "\n",
    "    try:\n",
    "        e_df = pd.read_csv(f\"{dataloc}{etf}.csv\", parse_dates=['date'])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"ETF file '{dataloc}{etf}.csv' not found.\")\n",
    "\n",
    "    # Merge DataFrames on 'date' to ensure alignment\n",
    "    merged_df = pd.merge(\n",
    "        s_df[s_df['date'] < cutoff_date],\n",
    "        e_df[e_df['date'] < cutoff_date],\n",
    "        on='date',\n",
    "        suffixes=('_stock', '_etf')\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Check if merge was successful\n",
    "    if merged_df.empty:\n",
    "        raise ValueError(f\"No overlapping dates found for stock '{stock}' and ETF '{etf}' before {cutoff_date}.\")\n",
    "\n",
    "    # Extract necessary columns as NumPy arrays for efficient processing\n",
    "    s_logclose = np.log(merged_df['AdjClose_stock'].values)\n",
    "    e_logclose = np.log(merged_df['AdjClose_etf'].values)\n",
    "    s_logopen = np.log(merged_df['AdjOpen_stock'].values)\n",
    "    e_logopen = np.log(merged_df['AdjOpen_etf'].values)\n",
    "\n",
    "    # Interleave open and close log prices using vectorized operations\n",
    "    s_log = np.empty(2 * len(s_logclose))\n",
    "    e_log = np.empty(2 * len(e_logclose))\n",
    "    s_log[0::2] = s_logopen\n",
    "    s_log[1::2] = s_logclose\n",
    "    e_log[0::2] = e_logopen\n",
    "    e_log[1::2] = e_logclose\n",
    "\n",
    "    # Calculate log returns\n",
    "    s_ret = np.diff(s_log)\n",
    "    e_ret = np.diff(e_log)\n",
    "\n",
    "    # Cap returns to mitigate the effect of outliers\n",
    "    cap_value = 0.15\n",
    "    s_ret = np.clip(s_ret, -cap_value, cap_value)\n",
    "    e_ret = np.clip(e_ret, -cap_value, cap_value)\n",
    "\n",
    "    # Calculate ETF-excess returns\n",
    "    excessret = s_ret - e_ret\n",
    "\n",
    "    # Align dates: since returns are based on differences, exclude the first date\n",
    "    dates_dt = merged_df['date'].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    if plotcheck:\n",
    "        # Plot Adjusted Close Price\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(merged_df['date'], merged_df['AdjClose_stock'], label=f'{stock} AdjClose', color='blue')\n",
    "        plt.title(f'{stock} Adjusted Close Price')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot Returns\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(dates_dt, s_ret, alpha=0.7, label='Stock Returns', color='green')\n",
    "        plt.plot(dates_dt, e_ret, alpha=0.7, label='ETF Returns', color='orange')\n",
    "        plt.plot(dates_dt, excessret, alpha=0.7, label='Excess Returns', color='red')\n",
    "        plt.title(f'Returns for {stock} vs {etf}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Log Return')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return excessret, dates_dt\n",
    "\n",
    "def rawreturns(dataloc, stock, plotcheck = False):\n",
    "    \"\"\"\n",
    "    function to get a time series of raw log returns for a given stock/etf\n",
    "    all prices are adjusted for stock events\n",
    "    input: location of datasets, stock ticker, etf ticker\n",
    "    output: time series of etf excess log returns\n",
    "    optional: plot sanity check\n",
    "    \"\"\"\n",
    "    s_df = pd.read_csv(dataloc+stock+\".csv\")\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    d1 = pd.to_datetime(\"2022-01-01\")\n",
    "    smp = (dates_dt < d1)\n",
    "    s_df = s_df[smp]\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    s_logclose = np.log(s_df['AdjClose'])\n",
    "    s_logopen = np.log(s_df['AdjOpen'])\n",
    "    s_log = np.zeros(2*len(s_logclose))\n",
    "    for i in range(len(s_logclose)):\n",
    "        s_log[2 * i] = s_logopen[i]\n",
    "        s_log[2 * i + 1] = s_logclose[i]\n",
    "    s_ret = np.diff(s_log)\n",
    "    s_ret[s_ret > 0.15] = 0.15\n",
    "    s_ret[s_ret < -0.15] = -0.15\n",
    "    dates_dt = pd.to_datetime(s_df['date'])\n",
    "    if plotcheck:\n",
    "        plt.figure(stock+\" price\")\n",
    "        plt.title(stock+\" price\")\n",
    "        plt.plot(dates_dt,s_df['AdjClose'])\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.ylabel(\"price in USD\")\n",
    "        plt.show()\n",
    "        plt.figure(\"Returns \"+stock)\n",
    "        plt.title(\"Returns \"+stock)\n",
    "        plt.plot(range(len(s_ret)),s_ret)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return s_ret, dates_dt\n",
    "\n",
    "def split_train_val_test(stock, dataloc, etflistloc, tr = 0.8, vl = 0.1, h = 1, l = 10, pred = 1, plotcheck=False):\n",
    "    \"\"\"\n",
    "    prepare etf excess log returns for a given stock\n",
    "    split into train, val, test\n",
    "    h: sliding window\n",
    "    l: condition window (number of previous values)\n",
    "    pred: prediction window\n",
    "    \"\"\"\n",
    "    etf = ETF_find(etflistloc, stock)\n",
    "    excess_returns, dates_dt = excessreturns(dataloc, stock, etf, plotcheck)\n",
    "    N = len(excess_returns)\n",
    "    N_tr = int(tr*N)\n",
    "    N_vl = int(vl*N)\n",
    "    N_tst = N - N_tr - N_vl\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    test_sr = excess_returns[N_tr+N_vl:]\n",
    "    n = int((N_tr-l-pred)/h)+1\n",
    "    train_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        train_data[i,:] = train_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_vl-l-pred)/h)+1\n",
    "    val_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        val_data[i,:] = val_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_tst-l-pred)/h)+1\n",
    "    test_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        test_data[i,:] = test_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    if plotcheck:\n",
    "        plt.figure(\"Excess returns\")\n",
    "        plt.plot(dates_dt,excess_returns)\n",
    "        plt.title(stock+ \" excess returns\")\n",
    "        plt.axvline(x = dates_dt[N_tr],color = \"red\")\n",
    "        plt.axvline(x = dates_dt[N_tr+N_vl],color = \"red\")\n",
    "        plt.show()\n",
    "    return train_data,val_data,test_data, dates_dt\n",
    "\n",
    "def split_train_testraw(stock, dataloc, tr = 0.8, vl = 0.1, h = 1, l = 10, pred = 1, plotcheck=False):\n",
    "    \"\"\"\n",
    "    prepare raw log returns for a given stock\n",
    "    split into train, test\n",
    "    h: sliding window\n",
    "    l: condition window (number of previous values)\n",
    "    pred: prediction window\n",
    "    \"\"\"\n",
    "    excess_returns, dates_dt = rawreturns(dataloc, stock, plotcheck)\n",
    "    N = len(excess_returns)\n",
    "    N_tr = int(tr*N) + int(vl*N)\n",
    "    N_tst = N - N_tr\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    test_sr = excess_returns[N_tr:]\n",
    "    n = int((N_tr-l-pred)/h)+1\n",
    "    train_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        train_data[i,:] = train_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_tst-l-pred)/h)+1\n",
    "    test_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        test_data[i,:] = test_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "\n",
    "    return train_data,test_data\n",
    "\n",
    "\n",
    "def split_train_val_testraw(stock, dataloc, tr = 0.8, vl = 0.1, h = 1, l = 10, pred = 1, plotcheck=False):\n",
    "    \"\"\"\n",
    "    prepare raw log returns for a given stock\n",
    "    split into train, val, test\n",
    "    h: sliding window\n",
    "    l: condition window (number of previous values)\n",
    "    pred: prediction window\n",
    "    \"\"\"\n",
    "    excess_returns, dates_dt = rawreturns(dataloc, stock, plotcheck)\n",
    "    N = len(excess_returns)\n",
    "    N_tr = int(tr*N)\n",
    "    N_vl = int(vl*N)\n",
    "    N_tst = N - N_tr - N_vl\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    train_sr = excess_returns[0:N_tr]\n",
    "    val_sr = excess_returns[N_tr:N_tr+N_vl]\n",
    "    test_sr = excess_returns[N_tr+N_vl:]\n",
    "    n = int((N_tr-l-pred)/h)+1\n",
    "    train_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        train_data[i,:] = train_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_vl-l-pred)/h)+1\n",
    "    val_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        val_data[i,:] = val_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    n = int((N_tst-l-pred)/h)+1\n",
    "    test_data = np.zeros(shape=(n,l+pred))\n",
    "    l_tot = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        test_data[i,:] = test_sr[l_tot:l_tot+l+pred]\n",
    "        l_tot = l_tot + h\n",
    "    if plotcheck:\n",
    "        plt.figure(\"returns\")\n",
    "        plt.plot(dates_dt,excess_returns)\n",
    "        plt.title(stock+ \" =returns\")\n",
    "        plt.axvline(x = dates_dt[N_tr],color = \"red\")\n",
    "        plt.axvline(x = dates_dt[N_tr+N_vl],color = \"red\")\n",
    "        plt.show()\n",
    "    return train_data,val_data,test_data, dates_dt\n",
    "\n",
    "#LSTM ForGAN generator\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        noise_dim: the dimension of the noise, a scalar\n",
    "        cond_dim: the dimension of the condition, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, noise_dim,cond_dim, hidden_dim,output_dim,mean,std):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = noise_dim+cond_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        #predicting a single value, so the output dimension is 1\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        #Add the modules\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=cond_dim, hidden_size=self.hidden_dim, num_layers=1, dropout=0)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        self.linear1 = nn.Linear(in_features=self.hidden_dim+self.noise_dim, out_features=self.hidden_dim+self.noise_dim)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        self.linear2 = nn.Linear(in_features=self.hidden_dim+self.noise_dim, out_features=output_dim)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, noise,condition,h_0,c_0):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator:adding the noise and the condition separately\n",
    "        '''\n",
    "        #x = combine_vectors(noise.to(torch.float),condition.to(torch.float),2)\n",
    "        condition = (condition-self.mean)/self.std\n",
    "        out, (h_n, c_n) = self.lstm(condition, (h_0, c_0))\n",
    "        out = combine_vectors(noise.to(torch.float),h_n.to(torch.float),dim=-1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.linear2(out)\n",
    "        out = out*self.std+self.mean\n",
    "        return out\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    '''\n",
    "    Values:\n",
    "        cond_dim: the dimension of the condition, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, noise_dim,cond_dim, hidden_dim,output_dim,mean,std):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = noise_dim+cond_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        #predicting a single value, so the output dimension is 1\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        #Add the modules\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=cond_dim, hidden_size=self.output_dim, num_layers=1, dropout=0)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        # nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, condition,h_0,c_0):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator:adding the noise and the condition separately\n",
    "        '''\n",
    "        #x = combine_vectors(noise.to(torch.float),condition.to(torch.float),2)\n",
    "        condition = (condition-self.mean)/self.std\n",
    "        out, (h_n, c_n) = self.lstm(condition, (h_0, c_0))\n",
    "        out = out*self.std+self.mean\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "#discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "      in_dim: the input dimension (noise dim + conditin dim + forecast dim for the condition for this dataset), a scalar\n",
    "      hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, in_dim, hidden_dim,mean,std):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=self.hidden_dim, num_layers=1, dropout=0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_ih_l0)\n",
    "        nn.init.xavier_normal_(self.lstm.weight_hh_l0)\n",
    "        self.linear = nn.Linear(in_features=self.hidden_dim, out_features=1)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, in_chan,h_0,c_0):\n",
    "        '''\n",
    "        in_chan: concatenated condition with real or fake\n",
    "        h_0 and c_0: for the LSTM\n",
    "        '''\n",
    "        x = in_chan\n",
    "        x = (x-self.mean)/self.std\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        out = self.linear(h_n)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "def combine_vectors(x, y,dim=-1):\n",
    "    '''\n",
    "    Function for combining two tensors\n",
    "    '''\n",
    "    combined = torch.cat([x,y],dim=dim)\n",
    "    combined = combined.to(torch.float)\n",
    "    return combined\n",
    "\n",
    "def getPnL(predicted,real,nsamp):\n",
    "    \"\"\"\n",
    "    PnL per trade given nsamp samples, predicted forecast, real data realisations\n",
    "    in bpts\n",
    "    \"\"\"\n",
    "    sgn_fake = torch.sign(predicted)\n",
    "    PnL = torch.sum(sgn_fake*real)\n",
    "    PnL = 10000*PnL/nsamp\n",
    "    return PnL\n",
    "\n",
    "def getSR(predicted,real):\n",
    "    \"\"\"\n",
    "    Sharpe Ratio given forecasts predicted of real (not annualised)\n",
    "    \"\"\"\n",
    "    sgn_fake = torch.sign(predicted)\n",
    "    SR = torch.mean(sgn_fake * real) / torch.std(sgn_fake * real)\n",
    "    return SR\n",
    "\n",
    "def Evaluation2(ticker,freq,gen,test_data, val_data, h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, losstype, sr_val, device, plotsloc, f_name, plot = False):\n",
    "    \"\"\"\n",
    "    Evaluation of a GAN model on a single stock\n",
    "    \"\"\"\n",
    "    df_temp = False\n",
    "    dt = {'lrd':lrd,'lrg':lrg,'type': losstype,'epochs':n_epochs, 'ticker':ticker,  'hid_g':hid_g, 'hid_d':hid_d}\n",
    "    #print(\"Validation set best PnL (in bp): \",PnL_best)\n",
    "    #print(\"Checkpoint epoch: \",checkpoint_last_epoch+1)\n",
    "    ntest = test_data.shape[0]\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        condition1 = test_data[:,0:l]\n",
    "        condition1 = condition1.unsqueeze(0)\n",
    "        condition1 = condition1.to(device)\n",
    "        condition1 = condition1.to(torch.float)\n",
    "        ntest = test_data.shape[0]\n",
    "        h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "        fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "        fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "        generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "        generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "        #generated1 = fake1.detach()\n",
    "        for i in range(999):\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            #print(fake.shape)\n",
    "            generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "#             print(generated1.shape)\n",
    "            del fake1\n",
    "            del fake_noise\n",
    "        #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "        #mae = torch.mean(torch.abs(fake-real))\n",
    "    #print(\"RMSE: \", rmse)\n",
    "    #print(\"MAE: \",mae)\n",
    "    b1 = generated1.squeeze()\n",
    "    mn1 = torch.mean(b1,dim=1)\n",
    "    real1 = test_data[:,-1]\n",
    "    rl1 = real1.squeeze()\n",
    "    rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "    mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "    #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "    dt['RMSE'] = rmse1.item()\n",
    "    dt['MAE'] = mae1.item()\n",
    "    ft1 = mn1.clone().detach().to(device)\n",
    "    PnL1 = getPnL(ft1,rl1,ntest)\n",
    "    #print(\"PnL in bp\", PnL)\n",
    "\n",
    "    #look at the Sharpe Ratio\n",
    "    n_b1 = b1.shape[1]\n",
    "    PnL_ws1 = torch.empty(ntest)\n",
    "    for i1 in range(ntest):\n",
    "        fk1 = b1[i1,:]\n",
    "        pu1 = (fk1>=0).sum()\n",
    "        pu1 = pu1/n_b1\n",
    "        pd1 = 1-pu1\n",
    "        PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "        PnL_ws1[i1] = PnL_temp1.item()\n",
    "    PnL_ws1 = np.array(PnL_ws1)\n",
    "    PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    PnL_even = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    PnL_odd = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    for i1 in range(len(PnL_wd1)):\n",
    "        PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "        PnL_even[i1] = PnL_ws1[2 * i1]\n",
    "        PnL_odd[i1] = PnL_ws1[2 * i1 + 1]\n",
    "    PnL_test = PnL_wd1\n",
    "    PnL_w_m1 = np.mean(PnL_wd1)\n",
    "    PnL_w_std1 = np.std(PnL_wd1)\n",
    "    SR1 = PnL_w_m1/PnL_w_std1\n",
    "    #print(\"Sharpe Ratio: \",SR)\n",
    "    dt['SR_w scaled'] = SR1*np.sqrt(252)\n",
    "    dt['PnL_w'] = PnL_w_m1\n",
    "\n",
    "    if (ntest % 2) == 0:\n",
    "        dt['Close-to-Open SR_w'] = np.sqrt(252) * np.mean(PnL_even) / np.std(PnL_even)\n",
    "        dt['Open-to-Close SR_w'] = np.sqrt(252) * np.mean(PnL_odd) / np.std(PnL_odd)\n",
    "    else:\n",
    "        dt['Open-to-Close SR_w'] = np.sqrt(252) * np.mean(PnL_even) / np.std(PnL_even)\n",
    "        dt['Close-to-Open SR_w'] = np.sqrt(252) * np.mean(PnL_odd) / np.std(PnL_odd)\n",
    "    print(\"Annualised (test) SR_w: \",SR1*np.sqrt(252))\n",
    "\n",
    "    distcheck = np.array(b1[1,:].cpu())\n",
    "    means = np.array(mn1.detach())\n",
    "    reals = np.array(rl1.detach())\n",
    "    dt['Corr'] = np.corrcoef([means,reals])[0,1]\n",
    "    dt['Pos mn'] = np.sum(means >0)/ len(means)\n",
    "    dt['Neg mn'] = np.sum(means <0)/ len(means)\n",
    "    print('Correlation ',np.corrcoef([means,reals])[0,1] )\n",
    "\n",
    "    dt['narrow dist'] = (np.std(distcheck)<0.0002)\n",
    "\n",
    "    means_gen = means\n",
    "    reals_test = reals\n",
    "    distcheck_test = distcheck\n",
    "    rl_test = reals[1]\n",
    "\n",
    "    mn = torch.mean(b1,dim=1)\n",
    "    mn = np.array(mn.cpu())\n",
    "    dt['narrow means dist'] = (np.std(mn)<0.0002)\n",
    "\n",
    "    ntest = val_data.shape[0]\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        condition1 = val_data[:,0:l]\n",
    "        condition1 = condition1.unsqueeze(0)\n",
    "        condition1 = condition1.to(device)\n",
    "        condition1 = condition1.to(torch.float)\n",
    "        ntest = val_data.shape[0]\n",
    "        h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "        fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "        fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "        generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "        generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "        #generated1 = fake1.detach()\n",
    "        for i in range(999):\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            #print(fake.shape)\n",
    "            generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "#             print(generated1.shape)\n",
    "            del fake1\n",
    "            del fake_noise\n",
    "        #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "        #mae = torch.mean(torch.abs(fake-real))\n",
    "    #print(\"RMSE: \", rmse)\n",
    "    #print(\"MAE: \",mae)\n",
    "    b1 = generated1.squeeze()\n",
    "    mn1 = torch.mean(b1,dim=1)\n",
    "    real1 = val_data[:,-1]\n",
    "    rl1 = real1.squeeze()\n",
    "    rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "    mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "    #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "    dt['RMSE val'] = rmse1.item()\n",
    "    dt['MAE val'] = mae1.item()\n",
    "    ft1 = mn1.clone().detach().to(device)\n",
    "    #print(\"PnL in bp\", PnL)\n",
    "\n",
    "    #look at the Sharpe Ratio\n",
    "    n_b1 = b1.shape[1]\n",
    "    PnL_ws1 = torch.empty(ntest)\n",
    "    for i1 in range(ntest):\n",
    "        fk1 = b1[i1,:]\n",
    "        pu1 = (fk1>=0).sum()\n",
    "        pu1 = pu1/n_b1\n",
    "        pd1 = 1-pu1\n",
    "        PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "        PnL_ws1[i1] = PnL_temp1.item()\n",
    "    PnL_ws1 = np.array(PnL_ws1)\n",
    "    PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "    for i1 in range(len(PnL_wd1)):\n",
    "        PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "    PnL_w_m1 = np.mean(PnL_wd1)\n",
    "    PnL_w_std1 = np.std(PnL_wd1)\n",
    "    SR1 = PnL_w_m1/PnL_w_std1\n",
    "    #print(\"Sharpe Ratio: \",SR)\n",
    "    dt['PnL_w val'] = PnL_w_m1\n",
    "    dt['SR_w scaled val'] = SR1*np.sqrt(252)\n",
    "\n",
    "    print(\"Annualised (val) SR_w : \",SR1*np.sqrt(252))\n",
    "\n",
    "    means = np.array(mn1.detach())\n",
    "    reals = np.array(rl1.detach())\n",
    "    dt['Corr val'] = np.corrcoef([means,reals])[0,1]\n",
    "    dt['Pos mn val'] = np.sum(means >0)/ len(means)\n",
    "    dt['Neg mn val'] = np.sum(means <0)/ len(means)\n",
    "\n",
    "    df_temp = pd.DataFrame(data=dt,index=[0])\n",
    "\n",
    "    return df_temp, PnL_test, PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test\n",
    "\n",
    "def Evaluation3(tickers,freq,gen,test, val, h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, losstype, sr_val, device, plotsloc, f_name, plot = False):\n",
    "    \"\"\"\n",
    "    Evaluation of a GAN model in the universality setting (multiple tickers)\n",
    "    \"\"\"\n",
    "    df_temp = False\n",
    "    dt = {'lrd':[],'lrg':[],'type': [],'epochs':[], 'ticker':[],  'hid_g':[], 'hid_d':[]}\n",
    "    results_df = pd.DataFrame(data = dt)\n",
    "    PnLs_test = np.zeros((len(tickers), int(0.5 * test[0].shape[0])))\n",
    "    PnLs_val = np.zeros((len(tickers), int(0.5 * val[0].shape[0])))\n",
    "    means_test = np.zeros((len(tickers), test[0].shape[0]))\n",
    "    means_val = np.zeros((len(tickers), val[0].shape[0]))\n",
    "    # print(means_test.shape)\n",
    "    #print(\"Validation set best PnL (in bp): \",PnL_best)\n",
    "    #print(\"Checkpoint epoch: \",checkpoint_last_epoch+1)\n",
    "    for ii in tqdm(range(len(tickers))):\n",
    "        val_data = val[ii]\n",
    "        test_data = test[ii]\n",
    "        ticker = tickers[ii]\n",
    "        dt = {'lrd':lrd,'lrg':lrg,'type': losstype,'epochs':n_epochs, 'ticker':ticker,  'hid_g':hid_g, 'hid_d':hid_d}\n",
    "        ntest = test_data.shape[0]\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            condition1 = test_data[:,0:l]\n",
    "            condition1 = condition1.unsqueeze(0)\n",
    "            condition1 = condition1.to(device)\n",
    "            condition1 = condition1.to(torch.float)\n",
    "            ntest = test_data.shape[0]\n",
    "            h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "            generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = fake1.detach()\n",
    "            for i in range(999):\n",
    "                fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "                fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "                fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "                #print(fake.shape)\n",
    "                generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "                #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "    #             print(generated1.shape)\n",
    "                del fake1\n",
    "                del fake_noise\n",
    "            #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "            #mae = torch.mean(torch.abs(fake-real))\n",
    "        #print(\"RMSE: \", rmse)\n",
    "        #print(\"MAE: \",mae)\n",
    "        b1 = generated1.squeeze()\n",
    "        mn1 = torch.mean(b1,dim=1)\n",
    "        # print(mn1.shape)\n",
    "        means_test[ii, :] = np.array(mn1.detach())\n",
    "        real1 = test_data[:,-1]\n",
    "        rl1 = real1.squeeze()\n",
    "        rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "        mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "        #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "        dt['RMSE'] = rmse1.item()\n",
    "        dt['MAE'] = mae1.item()\n",
    "        ft1 = mn1.clone().detach().to(device)        #print(\"PnL in bp\", PnL)\n",
    "\n",
    "        #look at the Sharpe Ratio\n",
    "        n_b1 = b1.shape[1]\n",
    "        PnL_ws1 = torch.empty(ntest)\n",
    "        for i1 in range(ntest):\n",
    "            fk1 = b1[i1,:]\n",
    "            pu1 = (fk1>=0).sum()\n",
    "            pu1 = pu1/n_b1\n",
    "            pd1 = 1-pu1\n",
    "            PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "            PnL_ws1[i1] = PnL_temp1.item()\n",
    "        PnL_ws1 = np.array(PnL_ws1)\n",
    "        PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "        for i1 in range(len(PnL_wd1)):\n",
    "            PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "        PnLs_test[ii, :] = PnL_wd1\n",
    "        PnL_w_m1 = np.mean(PnL_wd1)\n",
    "        PnL_w_std1 = np.std(PnL_wd1)\n",
    "        SR1 = PnL_w_m1/PnL_w_std1\n",
    "        #print(\"Sharpe Ratio: \",SR)\n",
    "        dt['PnL_w'] = PnL_w_m1\n",
    "        dt['SR_w scaled'] = SR1 * np.sqrt(252)\n",
    "        # print(\"Annualised (test) SR_w: \",SR1*np.sqrt(252 * freq))\n",
    "        # print(\"Annualised (test) SR_m: \", np.sqrt(252 * freq) * getSR(ft1,rl1).item())\n",
    "        dist_loc = plotsloc+\"distcheck-\"+f_name+\".png\"\n",
    "\n",
    "        distcheck = np.array(b1[1,:].cpu())\n",
    "        means = np.array(mn1.detach())\n",
    "        reals = np.array(rl1.detach())\n",
    "        dt['Corr'] = np.corrcoef([means,reals])[0,1]\n",
    "        dt['Pos mn'] = np.sum(means >0)/ len(means)\n",
    "        dt['Neg mn'] = np.sum(means <0)/ len(means)\n",
    "        # print('Correlation ',np.corrcoef([means,reals])[0,1] )\n",
    "\n",
    "        dt['narrow dist'] = (np.std(distcheck)<0.0002)\n",
    "\n",
    "        means_loc = plotsloc+\"recovered-means-\"+f_name+\".png\"\n",
    "\n",
    "\n",
    "        mn = torch.mean(b1,dim=1)\n",
    "        mn = np.array(mn.cpu())\n",
    "        dt['narrow means dist'] = (np.std(mn)<0.0002)\n",
    "\n",
    "\n",
    "        ntest = val_data.shape[0]\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            condition1 = val_data[:,0:l]\n",
    "            condition1 = condition1.unsqueeze(0)\n",
    "            condition1 = condition1.to(device)\n",
    "            condition1 = condition1.to(torch.float)\n",
    "            ntest = val_data.shape[0]\n",
    "            h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "            fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "            fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "            fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            generated1 = torch.empty([1,1,1,ntest,1000])\n",
    "            generated1[0,0,0,:,0] = fake1[0,0,0,:,0].detach()\n",
    "            #generated1 = fake1.detach()\n",
    "            for i in range(999):\n",
    "                fake_noise = torch.randn(1,ntest, z_dim, device=device,dtype=torch.float)\n",
    "                fake1 = gen(fake_noise,condition1,h0,c0)\n",
    "                fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "                #print(fake.shape)\n",
    "                generated1[0, 0, 0, :, i+1] = fake1[0,0,0,:,0].detach()\n",
    "                #generated1 = combine_vectors(generated1, fake1.detach(), dim=-1)\n",
    "    #             print(generated1.shape)\n",
    "                del fake1\n",
    "                del fake_noise\n",
    "            #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "            #mae = torch.mean(torch.abs(fake-real))\n",
    "        #print(\"RMSE: \", rmse)\n",
    "        #print(\"MAE: \",mae)\n",
    "        b1 = generated1.squeeze()\n",
    "        mn1 = torch.mean(b1,dim=1)\n",
    "        means_val[ii, :] = np.array(mn1.detach())\n",
    "\n",
    "        real1 = val_data[:,-1]\n",
    "        rl1 = real1.squeeze()\n",
    "        rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "        mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "        #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "        dt['RMSE val'] = rmse1.item()\n",
    "        dt['MAE val'] = mae1.item()\n",
    "        ft1 = mn1.clone().detach().to(device)\n",
    "        #print(\"PnL in bp\", PnL)\n",
    "\n",
    "        #look at the Sharpe Ratio\n",
    "        n_b1 = b1.shape[1]\n",
    "        PnL_ws1 = torch.empty(ntest)\n",
    "        for i1 in range(ntest):\n",
    "            fk1 = b1[i1,:]\n",
    "            pu1 = (fk1>=0).sum()\n",
    "            pu1 = pu1/n_b1\n",
    "            pd1 = 1-pu1\n",
    "            PnL_temp1 = 10000*(pu1*rl1[i1].item()-pd1*rl1[i1].item())\n",
    "            PnL_ws1[i1] = PnL_temp1.item()\n",
    "        PnL_ws1 = np.array(PnL_ws1)\n",
    "\n",
    "        PnL_wd1 = np.zeros(int(0.5 * len(PnL_ws1)))\n",
    "        for i1 in range(len(PnL_wd1)):\n",
    "            PnL_wd1[i1] = PnL_ws1[2 * i1] + PnL_ws1[2 * i1 + 1]\n",
    "        PnLs_val[ii, :] = PnL_wd1\n",
    "        PnL_w_m1 = np.mean(PnL_wd1)\n",
    "        PnL_w_std1 = np.std(PnL_wd1)\n",
    "        SR1 = PnL_w_m1/PnL_w_std1\n",
    "        #print(\"Sharpe Ratio: \",SR)\n",
    "        dt['PnL_w val'] = PnL_w_m1\n",
    "        dt['SR_w scaled val'] = SR1*np.sqrt(freq)\n",
    "\n",
    "\n",
    "        # print(\"Annualised (val) SR_w : \",SR1*np.sqrt(252 * freq))\n",
    "        # print(\"Annualised (val) SR_m : \", np.sqrt(252 * freq) * getSR(ft1,rl1).item())\n",
    "\n",
    "        means = np.array(mn1.detach())\n",
    "        reals = np.array(rl1.detach())\n",
    "        dt['Corr val'] = np.corrcoef([means,reals])[0,1]\n",
    "        dt['Pos mn val'] = np.sum(means >0)/ len(means)\n",
    "        dt['Neg mn val'] = np.sum(means <0)/ len(means)\n",
    "        df_temp = pd.DataFrame(data=dt,index=[0])\n",
    "        results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    PnL_test = np.sum(PnLs_test,axis=0)\n",
    "    PnL_val = np.sum(PnLs_val,axis=0)\n",
    "\n",
    "    return results_df, PnL_test, PnL_val, means_test, means_val\n",
    "\n",
    "def GradientCheck(ticker, gen, disc, gen_opt, disc_opt, criterion, n_epochs, train_data,batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Gradient norm check\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    BCE_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    PnL_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    MSE_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    SR_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    STD_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "    #currstep = 0\n",
    "    #train the discriminator more\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            MSE = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "            STD = torch.std(PnL_s)\n",
    "            gen_opt.zero_grad()\n",
    "            SR.backward(retain_graph=True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "            #list of gradient norms\n",
    "            SR_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            PnL.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            PnL_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            MSE.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            MSE_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            STD.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            STD_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            gen_loss.backward()\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            BCE_norm[epoch*nbatches+i] = total_norm\n",
    "            gen_opt.step()\n",
    "\n",
    "\n",
    "    alpha = torch.mean(BCE_norm / PnL_norm)\n",
    "    beta =  torch.mean(BCE_norm / MSE_norm)\n",
    "    gamma =  torch.mean(BCE_norm / SR_norm)\n",
    "    delta = torch.mean(BCE_norm / STD_norm)\n",
    "    print(\"Completed. \")\n",
    "    print(r\"$\\alpha$:\", alpha)\n",
    "    print(r\"$\\beta$:\", beta)\n",
    "    print(r\"$\\gamma$:\", gamma)\n",
    "    print(r\"$\\delta$:\", delta)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(ticker + \" BCE norm\")\n",
    "        plt.title(ticker + \" BCE norm\")\n",
    "        plt.plot(range(len(BCE_norm)),BCE_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" PnL norm\")\n",
    "        plt.title(ticker +\" PnL norm\")\n",
    "        plt.plot(range(len(BCE_norm)),PnL_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" MSE norm\")\n",
    "        plt.title(ticker + \" MSE norm\")\n",
    "        plt.plot(range(len(BCE_norm)), MSE_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" SR norm\")\n",
    "        plt.title(\"SR norm\")\n",
    "        plt.plot(range(len(BCE_norm)),SR_norm)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" STD norm\")\n",
    "        plt.title(ticker + \" STD norm\")\n",
    "        plt.plot(range(len(BCE_norm)),STD_norm)\n",
    "        plt.ylabel(r\"$L^2$ norm\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.show()\n",
    "\n",
    "        # plt.figure(ticker + \" Norms\")\n",
    "        # plt.title(ticker + \" gradient norms\")\n",
    "        # plt.plot(range(len(BCE_norm)),BCE_norm, label = \"BCE\")\n",
    "        # plt.plot(range(len(BCE_norm)),PnL_norm, label = \"PnL\")\n",
    "        # plt.plot(range(len(BCE_norm)),SR_norm, label = \"SR\")\n",
    "        # plt.plot(range(len(BCE_norm)),STD_norm, label = \"STD\")\n",
    "        # plt.ylabel(r\"$L^2$ norm\")\n",
    "        # plt.xlabel(\"iteration\")\n",
    "        # plt.legend(loc = 'best')\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "    return gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta\n",
    "\n",
    "\n",
    "def TrainLoopForGAN(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for the BCE GAN (ForGAN)\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed, checkpoint epoch: \", checkpoint_last_epoch)\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot=False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL and MSE loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "    SR_best = 0\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + beta * SqLoss\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed \")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def TrainLoopMainPnLMSESRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL, MSE, SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + beta * SqLoss - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "def TrainLoopMainPnLMSESTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL, MSE, STD loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + beta * SqLoss + delta * torch.std(PnL_s)\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "def TrainLoopMainPnLSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "    SR_best = 0\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "def TrainLoopMainMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: MSE loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))  + beta * SqLoss\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "def TrainLoopMainSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "def TrainLoopMainSRMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: SR, MSE loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) + beta * SqLoss - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "def TrainLoopMainPnLSTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop: PnL, STD loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    discloss = [False] * (nbatches*n_epochs)\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    dscpred_real = [False] * (nbatches*n_epochs)\n",
    "    dscpred_fake = [False] * (nbatches*n_epochs)\n",
    "    PnL_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "    SR_best = 0\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "            for j in range(diter):\n",
    "                disc_opt.zero_grad()\n",
    "            # Get noise corresponding to the current batch_size\n",
    "                noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "            # Get outputs from the generator\n",
    "                fake = gen(noise,condition,h_0g,c_0g)\n",
    "                # fake = fake.unsqueeze(0)\n",
    "                fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "                fake_and_condition.to(torch.float)\n",
    "                real_and_condition = combine_vectors(condition,real,dim=-1)\n",
    "\n",
    "                disc_fake_pred = disc(fake_and_condition.detach(),h_0d,c_0d)\n",
    "                disc_real_pred = disc(real_and_condition,h_0d,c_0d)\n",
    "\n",
    "            #Updating the discriminator\n",
    "\n",
    "                disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "                disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "                disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "                #disc_loss.backward(retain_graph=True)\n",
    "                disc_loss.backward()\n",
    "                disc_opt.step()\n",
    "\n",
    "            dscr = disc_real_pred[0][0][0].detach().item()\n",
    "            dscfk = disc_fake_pred[0][0][0].detach().item()\n",
    "            dscpred_real[epoch*nbatches+i] = dscr\n",
    "            dscpred_fake[epoch*nbatches+i] = dscfk\n",
    "\n",
    "            #fksmpl.append(fake.detach())\n",
    "            #rlsmpl.append(real.detach())\n",
    "\n",
    "\n",
    "            # Get the predictions from the discriminator\n",
    "\n",
    "\n",
    "\n",
    "            dloss = disc_loss.detach().item()\n",
    "            discloss[epoch*nbatches+i] = dloss\n",
    "\n",
    "\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            noise = torch.randn(1,curr_batch_size, z_dim, device=device,dtype=torch.float)\n",
    "\n",
    "\n",
    "            fake = gen(noise,condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "            fake_and_condition = combine_vectors(condition,fake,dim=-1)\n",
    "\n",
    "            disc_fake_pred = disc(fake_and_condition,h_0d,c_0d)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            STD = torch.std(PnL_s)\n",
    "\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) - alpha * PnL + delta * STD\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(\"Disc pred PnL STD\")\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_fake, alpha = 0.5, label = 'generated')\n",
    "        plt.plot(range(len(dscpred_fake)), dscpred_real, alpha = 0.5, label = 'real')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Gen loss PnL STD\")\n",
    "        plt.title(\"Gen loss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Disc loss PnL STD\")\n",
    "        plt.title(\"Disc loss\")\n",
    "        plt.plot(range(len(discloss)),discloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "\n",
    "    return gen, disc, gen_opt, disc_opt\n",
    "\n",
    "def FinGAN_combos(ticker,loc,modelsloc,plotsloc,dataloc, etflistloc,  vl_later = True, lrg = 0.0001, lrd = 0.0001, n_epochs = 500, ngrad = 100, h = 1, l = 10, pred = 1, ngpu = 1, tanh_coeff = 100, tr = 0.8, vl = 0.1, z_dim = 32, hid_d = 64, hid_g = 8, checkpoint_epoch = 20, batch_size = 100, diter = 1, plot = False, freq = 2):\n",
    "    \"\"\"\n",
    "    FinGAN: looking at all combinations, performance on both validation and test set for all\n",
    "    \"\"\"\n",
    "    #initialise the networks first:\n",
    "    datastart = {'lrd':[],'lrg':[],'epochs':[],'SR_val':[]}\n",
    "    results_df = pd.DataFrame(data=datastart)\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "    if ticker[0] == 'X':\n",
    "        train_data,val_data,test_data, dates_dt = split_train_val_testraw(ticker, dataloc, tr, vl, h, l, pred, plotcheck = False)\n",
    "    else:\n",
    "        train_data,val_data,test_data, dates_dt = split_train_val_test(ticker, dataloc, etflistloc,  tr, vl, h, l, pred, plotcheck = False)\n",
    "    data_tt = torch.from_numpy(train_data)\n",
    "    train_data = data_tt.to(torch.float).to(device)\n",
    "    data_tt = torch.from_numpy(test_data)\n",
    "    test_data = data_tt.to(torch.float).to(device)\n",
    "    data_tt = torch.from_numpy(val_data)\n",
    "    validation_data = data_tt.to(torch.float).to(device)\n",
    "    ntest = test_data.shape[0]\n",
    "    condition_size = l\n",
    "    target_size = pred\n",
    "    ref_mean = torch.mean(train_data[0:batch_size,:])\n",
    "    ref_std = torch.std(train_data[0:batch_size,:])\n",
    "    discriminator_indim = condition_size+target_size\n",
    "\n",
    "    gen = Generator(noise_dim=z_dim,cond_dim=condition_size, hidden_dim=hid_g,output_dim=pred,mean =ref_mean,std=ref_std)\n",
    "    gen.to(device)\n",
    "\n",
    "    disc = Discriminator(in_dim=discriminator_indim, hidden_dim=hid_d,mean=ref_mean,std=ref_std)\n",
    "    disc.to(device)\n",
    "\n",
    "    gen_opt = torch.optim.RMSprop(gen.parameters(), lr=lrg)\n",
    "    disc_opt = torch.optim.RMSprop(disc.parameters(), lr=lrd)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta = GradientCheck(ticker, gen, disc, gen_opt, disc_opt, criterion, ngrad, train_data,batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "\n",
    "    f_name = modelsloc + ticker + \"-Fin-GAN-\"+str(n_epochs)+\"-epochs-\"+str(lrd)+\"-lrd-\"+str(lrg)+\"-lrg-\"\n",
    "    f_name1 = ticker + \"-Fin-GAN-\"+str(n_epochs)+\"-epochs-\"+str(lrd)+\"-lrd-\"+str(lrg)+\"-lrg\"\n",
    "    PnL_test = [False] * 10\n",
    "    print(\"PnL\")\n",
    "    losstype = \"PnL\"\n",
    "    genPnL, discPnL, gen_optPnL, disc_optPnL = TrainLoopMainPnLnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnL.state_dict()}, f_name + \"PnL_generator_checkpoint.pth\")\n",
    "    df_temp, PnL_test[0], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genPnL,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "\n",
    "    pd.DataFrame(PnL_test[0]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.title(\"Cummulative PnL \"+ticker)\n",
    "    plt.grid(visible=True)\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"bpts\")\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[0]), label = \"PnL\")\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.title(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.grid(visible=True)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.ylabel(\"bpts\")\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.title(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(visible=True)\n",
    "\n",
    "        plt.ylabel(\"bpts\")\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.title(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.grid(visible=True)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"bpts\")\n",
    "\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.title(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(visible=True)\n",
    "        plt.ylabel(\"bpts\")\n",
    "\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.title(\"Simulated distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50,density = True, stacked=True, label = \"PnL\")\n",
    "    plt.xlabel(\"excess return\")\n",
    "    plt.ylabel(\"density\")\n",
    "    plt.grid(visible=True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.axvline(rl_test, color='k', linestyle='dashed', linewidth = 2)\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.title(\"Simulated means \"+ticker)\n",
    "    plt.hist(reals_test, alpha = 0.6, bins = 100,density = True, stacked=True, label = \"True\")\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100, density = True, stacked=True,label = \"PnL\")\n",
    "    plt.xlabel(\"excess return\")\n",
    "    plt.ylabel(\"density\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(visible=True)\n",
    "\n",
    "\n",
    "    print(\"PnL MSE\")\n",
    "    losstype = \"PnL MSE\"\n",
    "    genPnLMSE, discPnLMSE, gen_optPnLMSE, disc_optPnLMSE = TrainLoopMainPnLMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLMSE.state_dict()}, f_name + \"PnLMSE_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[1], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genPnLMSE,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL MSE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "\n",
    "    pd.DataFrame(PnL_test[1]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[1]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50,density = True, stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True,stacked=True, label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    print(\"PnL MSE STD\")\n",
    "    losstype = \"PnL MSE STD\"\n",
    "    genPnLMSESTD, discPnLMSESTD, gen_optPnLMSESTD, disc_optPnLMSESTD = TrainLoopMainPnLMSESTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device,  plot)\n",
    "    torch.save({'g_state_dict': genPnLMSESTD.state_dict()}, f_name + \"PnLMSESTD_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[2], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genPnLMSESTD,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL MSE STD\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[2]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    pd.DataFrame(PnL_test[2]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50,density = True,stacked=True, label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True, stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL MSE SR\")\n",
    "    losstype = \"PnL MSE SR\"\n",
    "    genPnLMSESR, discPnLMSESR, gen_optPnLMSESR, disc_optPnLMSESR = TrainLoopMainPnLMSESRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device,  plot)\n",
    "    torch.save({'g_state_dict': genPnLMSESR.state_dict()}, f_name + \"PnLMSESR_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[3], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genPnLMSESR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL MSE SR\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[3]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    pd.DataFrame(PnL_test[3]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50,density = True,stacked=True, label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True, stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL SR\")\n",
    "    losstype = \"PnL SR\"\n",
    "    genPnLSR, discPnLSR, gen_optPnLSR, disc_optPnLSR = TrainLoopMainPnLSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device,  plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"PnLSR_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[4], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genPnLSR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL SR\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[4]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    pd.DataFrame(PnL_test[4]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50, density = True,stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100, density = True,stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL STD\")\n",
    "    losstype = \"PnL STD\"\n",
    "    genPnLSTD, discPnLSTD, gen_optPnLSTD, disc_optPnLSTD = TrainLoopMainPnLSTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device,  plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"PnLSTD_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[5], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genPnLSTD,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL STD\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[5]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    pd.DataFrame(PnL_test[5]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50,density = True,stacked=True, label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True,stacked=True, label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"SR\")\n",
    "    losstype = \"SR\"\n",
    "    genSR, discSR, gen_optSR, disc_optSR = TrainLoopMainSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device,  plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"SR_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[6], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genSR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"SR\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[6]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    pd.DataFrame(PnL_test[6]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50,density = True, stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True,stacked=True, label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"SR MSE\")\n",
    "    losstype = \"SR MSE\"\n",
    "    genSRMSE, discSRMSE, gen_optSRMSE, disc_optSRMSE = TrainLoopMainSRMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device,  plot)\n",
    "    torch.save({'g_state_dict': genSRMSE.state_dict()}, f_name + \"SRMSE_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[7], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genSRMSE,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"SR MSE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[7]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    pd.DataFrame(PnL_test[7]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50, density = True,stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True, stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"MSE\")\n",
    "    losstype = \"MSE\"\n",
    "    genMSE, discMSE, gen_optMSE, disc_optMSE = TrainLoopMainMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genMSE.state_dict()}, f_name + \"MSE_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[8], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genMSE,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"MSE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[8]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    pd.DataFrame(PnL_test[8]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50,density = True, stacked=True,label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True,stacked=True, label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"ForGAN\")\n",
    "    losstype = \"ForGAN\"\n",
    "    genfg, discfg, gen_optfg, disc_optfg = TrainLoopForGAN(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genfg.state_dict()}, f_name + \"ForGAN_generator_checkpoint.pth\")\n",
    "    df_temp,  PnL_test[9], PnL_even, PnL_odd, means_gen, reals_test, distcheck_test, rl_test = Evaluation2(ticker,freq,genfg,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"BCE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[9]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(plotsloc+ticker+\"-FinGAN-CummPnL.png\")\n",
    "    plt.show()\n",
    "    pd.DataFrame(PnL_test[9]).to_csv(loc+\"PnLs/\"+ticker+\"-FinGAN-\"+losstype+\".csv\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.savefig(plotsloc+ticker+\"-FinGAN-intradaycummPnL.png\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.savefig(plotsloc+ticker+\"-FinGAN-overnightcummPnL.png\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.savefig(plotsloc+ticker+\"-FinGAN-overnightcummPnL.png\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even),label = losstype)\n",
    "        plt.savefig(plotsloc+ticker+\"-FinGAN-intradaycummPnL.png\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(\"Sample distribution \"+ticker)\n",
    "    plt.hist(distcheck_test,alpha = 0.5, bins=50, density = True,stacked=True, label = losstype)\n",
    "    plt.savefig(plotsloc+ticker+\"-FinGAN-sample-dist.png\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(\"Means \"+ticker)\n",
    "    plt.hist(means_gen,alpha = 0.5, bins=100,density = True, stacked=True,label = losstype)\n",
    "    plt.savefig(plotsloc+ticker+\"-FinGAN-means.png\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    corr_m = np.corrcoef(PnL_test)\n",
    "\n",
    "    # can return tge best (validation) generator here too\n",
    "\n",
    "    return results_df,corr_m\n",
    "\n",
    "\n",
    "def GradientCheckLSTM(ticker, gen, gen_opt, n_epochs, train_data,batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Gradient check for LSTM-Fin\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    PnL_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    MSE_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    SR_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "    STD_norm = torch.empty(nbatches*n_epochs, device = device)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    disc_fake_pred = False\n",
    "    disc_real_pred = False\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "    #currstep = 0\n",
    "    #train the discriminator more\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            fake = gen(condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            MSE = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "            SR = (torch.mean(PnL_s)) / (torch.std(PnL_s))\n",
    "            STD = torch.std(PnL_s)\n",
    "            gen_opt.zero_grad()\n",
    "            SR.backward(retain_graph=True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "            #list of gradient norms\n",
    "            SR_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            PnL.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            PnL_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            STD.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            STD_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            MSE.backward(retain_graph = True)\n",
    "            total_norm = 0\n",
    "            for p in gen.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** 0.5\n",
    "            MSE_norm[epoch*nbatches+i] = total_norm\n",
    "\n",
    "            gen_opt.step()\n",
    "\n",
    "\n",
    "    alpha = torch.mean(MSE_norm / PnL_norm)\n",
    "    beta =  0\n",
    "    gamma =  torch.mean(MSE_norm / SR_norm)\n",
    "    delta = torch.mean(MSE_norm / STD_norm)\n",
    "    print(\"Completed. \")\n",
    "    print(r\"$\\alpha$:\", alpha)\n",
    "    print(r\"$\\beta$:\", beta)\n",
    "    print(r\"$\\gamma$:\", gamma)\n",
    "    print(r\"$\\delta$:\", delta)\n",
    "\n",
    "    if plot:\n",
    "\n",
    "\n",
    "        plt.figure(ticker + \" PnL norm\")\n",
    "        plt.title(\"PnL norm\")\n",
    "        plt.plot(range(len(MSE_norm)),PnL_norm)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" MSE norm\")\n",
    "        plt.title(\"MSE norm\")\n",
    "        plt.plot(range(len(MSE_norm)), MSE_norm)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" SR norm\")\n",
    "        plt.title(\"SR norm\")\n",
    "        plt.plot(range(len(MSE_norm)),SR_norm)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(ticker + \" std norm\")\n",
    "        plt.title(\"std norm\")\n",
    "        plt.plot(range(len(MSE_norm)),STD_norm)\n",
    "        plt.show()\n",
    "\n",
    "    return gen, gen_opt, alpha, beta, gamma, delta\n",
    "\n",
    "\n",
    "\n",
    "def Evaluation2LSTM(ticker,freq,gen,test_data, val_data, h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, losstype, sr_val, device, plotsloc, f_name, plot = False):\n",
    "    \"\"\"\n",
    "    LSTM(-FIn) evaluation on  a single stock\n",
    "    \"\"\"\n",
    "    df_temp = False\n",
    "    dt = {'lrd':lrd,'lrg':lrg,'type': losstype,'epochs':n_epochs, 'ticker':ticker}\n",
    "    #print(\"Validation set best PnL (in bp): \",PnL_best)\n",
    "    #print(\"Checkpoint epoch: \",checkpoint_last_epoch+1)\n",
    "    ntest = test_data.shape[0]\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        condition1 = test_data[:,0:l]\n",
    "        condition1 = condition1.unsqueeze(0)\n",
    "        condition1 = condition1.to(device)\n",
    "        condition1 = condition1.to(torch.float)\n",
    "        ntest = test_data.shape[0]\n",
    "        h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        fake1 = gen(condition1,h0,c0)\n",
    "        #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "        #mae = torch.mean(torch.abs(fake-real))\n",
    "    #print(\"RMSE: \", rmse)\n",
    "    #print(\"MAE: \",mae)\n",
    "    b1 = fake1[0,:,0]\n",
    "    mn1 = b1\n",
    "    real1 = test_data[:,-1]\n",
    "    rl1 = real1.squeeze()\n",
    "\n",
    "    rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "    mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "    #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "    dt['RMSE'] = rmse1.item()\n",
    "    dt['MAE'] = mae1.item()\n",
    "    ft1 = mn1.clone().detach().to(device)\n",
    "    PnL1 = getPnL(ft1,rl1,ntest)\n",
    "    #print(\"PnL in bp\", PnL)\n",
    "    PnLs = 10000 * np.sign(np.array(ft1.detach())) * np.array(rl1.detach())\n",
    "    PnLd = np.zeros(int(0.5*len(PnLs)))\n",
    "    PnL_even = np.zeros(int(0.5*len(PnLs)))\n",
    "    PnL_odd = np.zeros(int(0.5*len(PnLs)))\n",
    "    for i1 in range(len(PnLd)):\n",
    "        PnLd[i1] = PnLs[2*i1] + PnLs[2*i1+1]\n",
    "        PnL_even[i1] = PnLs[2*i1]\n",
    "        PnL_odd[i1] = PnLs[2 * i1 + 1]\n",
    "    PnL1 = np.mean(PnLd)\n",
    "    #print(\"PnL in bp\", PnL)\n",
    "    dt['PnL_m test'] = np.mean(PnLd)\n",
    "    PnL_test = PnLd\n",
    "\n",
    "    dt['SR_m scaled test'] = np.sqrt(252) * np.mean(PnLd) / np.std(PnLd)\n",
    "\n",
    "\n",
    "    print(\"Annualised (test) SR_m: \", np.sqrt(252) * np.mean(PnLd) / np.std(PnLd))\n",
    "\n",
    "    if (ntest % 2) == 0:\n",
    "        dt['Close-to-Open SR_w'] = np.sqrt(252) * np.mean(PnL_even) / np.std(PnL_even)\n",
    "        dt['Open-to-Close SR_w'] = np.sqrt(252) * np.mean(PnL_odd) / np.std(PnL_odd)\n",
    "    else:\n",
    "        dt['Open-to-Close SR_w'] = np.sqrt(252) * np.mean(PnL_even) / np.std(PnL_even)\n",
    "        dt['Close-to-Open SR_w'] = np.sqrt(252) * np.mean(PnL_odd) / np.std(PnL_odd)\n",
    "    means = np.array(mn1.detach())\n",
    "    reals = np.array(rl1.detach())\n",
    "    dt['Corr'] = np.corrcoef([means,reals])[0,1]\n",
    "    print('Correlation ', np.corrcoef([means,reals])[0,1])\n",
    "    dt['Pos mn'] = np.sum(means >0)/ len(means)\n",
    "    dt['Neg mn'] = np.sum(means <0)/ len(means)\n",
    "    ntest = val_data.shape[0]\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        condition1 = val_data[:,0:l]\n",
    "        condition1 = condition1.unsqueeze(0)\n",
    "        condition1 = condition1.to(device)\n",
    "        condition1 = condition1.to(torch.float)\n",
    "        ntest = val_data.shape[0]\n",
    "        h0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        c0 = torch.zeros((1,ntest,hid_g),device=device,dtype=torch.float)\n",
    "        fake1 = gen(condition1,h0,c0)\n",
    "\n",
    "        #rmse = torch.sqrt(torch.mean((fake-real)**2))\n",
    "        #mae = torch.mean(torch.abs(fake-real))\n",
    "    #print(\"RMSE: \", rmse)\n",
    "    #print(\"MAE: \",mae)\n",
    "    b1 = fake1[0,:,0]\n",
    "    mn1 = b1\n",
    "    real1 = val_data[:,-1]\n",
    "    rl1 = real1.squeeze()\n",
    "    rmse1 = torch.sqrt(torch.mean((mn1-rl1)**2))\n",
    "    mae1 = torch.mean(torch.abs(mn1-rl1))\n",
    "    #print(\"RMSE: \",rmse,\"MAE: \",mae)\n",
    "    dt['RMSE val'] = rmse1.item()\n",
    "    dt['MAE val'] = mae1.item()\n",
    "    ft1 = mn1.clone().detach().to(device)\n",
    "    PnLs = 10000 * np.sign(np.array(ft1.detach())) * np.array(rl1.detach())\n",
    "    PnLd = np.zeros(int(0.5*len(PnLs)))\n",
    "    for i1 in range(len(PnLd)):\n",
    "        PnLd[i1] = PnLs[2*i1] + PnLs[2*i1+1]\n",
    "    PnL1 = np.mean(PnLd)\n",
    "    #print(\"PnL in bp\", PnL)\n",
    "    dt['PnL_m val'] = PnL1\n",
    "\n",
    "    dt['SR_m scaled val'] = np.sqrt(252) * np.mean(PnLd) / np.std(PnLd)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Annualised (val) SR_m : \", np.sqrt(252 * freq) * getSR(ft1,rl1).item())\n",
    "    means = np.array(mn1.detach())\n",
    "    reals = np.array(rl1.detach())\n",
    "    dt['Corr val'] = np.corrcoef([means,reals])[0,1]\n",
    "    dt['Pos mn val'] = np.sum(means >0)/ len(means)\n",
    "    dt['Neg mn val'] = np.sum(means <0)/ len(means)\n",
    "    df_temp = pd.DataFrame(data=dt,index=[0])\n",
    "    return df_temp, PnL_test, PnL_even, PnL_odd\n",
    "\n",
    "\n",
    "\n",
    "def TrainLoopnLSTMPnL(gen, gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for LSTM-Fin with the PnL loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "\n",
    "            fake = gen(condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            gen_loss = SqLoss - alpha * PnL\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "\n",
    "\n",
    "        plt.figure(\"LSTM loss PnL\")\n",
    "        plt.title(\"LSTMloss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, gen_opt\n",
    "\n",
    "def TrainLoopnLSTMPnLSTD(gen, gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for LSTM-Fin with the PnL, STD loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "\n",
    "            fake = gen(condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            STD = torch.std(PnL_s)\n",
    "            gen_loss = SqLoss - alpha * PnL + delta * STD\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "\n",
    "\n",
    "        plt.figure(\"LSTM loss PnL STD\")\n",
    "        plt.title(\"LSTMloss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, gen_opt\n",
    "\n",
    "def TrainLoopnLSTMPnLSR(gen, gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for LSTM-Fin with the PnL,SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "\n",
    "            fake = gen(condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            PnL = torch.mean(PnL_s)\n",
    "            SR = torch.mean(PnL_s) / torch.std(PnL_s)\n",
    "            gen_loss = SqLoss - alpha * PnL - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "\n",
    "        plt.figure(\"LSTM loss PnL SR\")\n",
    "        plt.title(\"LSTMloss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, gen_opt\n",
    "\n",
    "def TrainLoopnLSTMSR(gen, gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for LSTM-Fin with the SR loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "\n",
    "            fake = gen(condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            SR = torch.mean(PnL_s) / torch.std(PnL_s)\n",
    "            gen_loss = SqLoss - gamma * SR\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "\n",
    "\n",
    "        plt.figure(\"LSTM loss SR\")\n",
    "        plt.title(\"LSTMloss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, gen_opt\n",
    "\n",
    "def TrainLoopnLSTMSTD(gen, gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for LSTM-Fin with the STD loss\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "\n",
    "            fake = gen(condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            sign_approx = torch.tanh(tanh_coeff * ft)\n",
    "            PnL_s  = sign_approx * rl\n",
    "            STD = torch.std(PnL_s)\n",
    "            gen_loss = SqLoss + delta * STD\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "\n",
    "\n",
    "        plt.figure(\"LSTM loss STD\")\n",
    "        plt.title(\"LSTMloss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, gen_opt\n",
    "\n",
    "def TrainLoopnLSTM(gen, gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lr_d = 0.0001, lr_g = 0.0001, h = 1, l = 10, pred = 1, diter =1, tanh_coeff = 100, device = 'cpu', plot = False):\n",
    "    \"\"\"\n",
    "    Training loop for LSTM\n",
    "    \"\"\"\n",
    "    ntrain = train_data.shape[0]\n",
    "    nval = validation_data.shape[0]\n",
    "    nbatches = ntrain//batch_size+1\n",
    "    genloss = [False] * (nbatches*n_epochs)\n",
    "\n",
    "    fake_and_condition = False\n",
    "    real_and_condition = False\n",
    "\n",
    "    totlen = train_data.shape[0]\n",
    "\n",
    "\n",
    "    #currstep = 0\n",
    "\n",
    "    #train the discriminator more\n",
    "\n",
    "    PnL_best = 0\n",
    "    SR_best = 0\n",
    "    checkpoint_last_epoch = 0\n",
    "\n",
    "    gen.train()\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        perm = torch.randperm(ntrain)\n",
    "        train_data = train_data[perm,:]\n",
    "        #shuffle the dataset for the optimisation to work\n",
    "        for i in range(nbatches):\n",
    "            curr_batch_size = batch_size\n",
    "            if i==(nbatches-1):\n",
    "                curr_batch_size = totlen-i*batch_size\n",
    "            h_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            c_0d = torch.zeros((1,curr_batch_size,hid_d),device=device,dtype= torch.float)\n",
    "            h_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "            c_0g = torch.zeros((1,curr_batch_size,hid_g),device=device,dtype= torch.float)\n",
    "\n",
    "            condition = train_data[(i*batch_size):(i*batch_size+curr_batch_size),0:l]\n",
    "            condition = condition.unsqueeze(0)\n",
    "            real = train_data[(i*batch_size):(i*batch_size+curr_batch_size),l:(l+pred)]\n",
    "            real = real.unsqueeze(0)\n",
    "\n",
    "\n",
    "            ### Update discriminator ###\n",
    "            # Zero out the discriminator gradients\n",
    "\n",
    "            # Update generator\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "\n",
    "            fake = gen(condition,h_0g,c_0g)\n",
    "\n",
    "            #fake1 = fake1.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "            ft = fake.squeeze(0).squeeze(1)\n",
    "            rl = real.squeeze(0).squeeze(1)\n",
    "            SqLoss = (torch.norm(ft-rl)**2) / curr_batch_size\n",
    "\n",
    "            gen_loss = SqLoss\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            gloss = gen_loss.detach().item()\n",
    "            genloss[epoch*nbatches+i] = gloss\n",
    "\n",
    "    if plot:\n",
    "\n",
    "\n",
    "        plt.figure(\"LSTM loss\")\n",
    "        plt.title(\"LSTMloss\")\n",
    "        plt.plot(range(len(genloss)),genloss)\n",
    "        plt.show()\n",
    "\n",
    "    # SR_best = SR_best * np.sqrt(252)\n",
    "    print(\"Training completed\")\n",
    "    # print(\"PnL val (best):\", PnL_best)\n",
    "    return gen, gen_opt\n",
    "\n",
    "def LSTM_combos(ticker,loc,modelsloc,plotsloc,dataloc, etflistloc,  vl_later = True, lrg = 0.0001, lrd = 0.0001, n_epochs = 500, ngrad = 100, h = 1, l = 10, pred = 1, ngpu = 1, tanh_coeff = 100, tr = 0.8, vl = 0.1, z_dim = 32, hid_d = 64, hid_g = 8, checkpoint_epoch = 20, batch_size = 100, diter = 1, plot = False, freq = 2):\n",
    "    \"\"\"\n",
    "    Training and evaluation on (test and val) of LSTM and LSTM-Fin\n",
    "    \"\"\"\n",
    "    #initialise the networks first:\n",
    "    datastart = {'lrd':[],'lrg':[],'epochs':[],'SR_val':[]}\n",
    "    results_df = pd.DataFrame(data=datastart)\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "    if ticker[0] == 'X':\n",
    "        train_data,val_data,test_data, dates_dt = split_train_val_testraw(ticker, dataloc, tr, vl, h, l, pred, plotcheck = False)\n",
    "    else:\n",
    "        train_data,val_data,test_data, dates_dt = split_train_val_test(ticker, dataloc, etflistloc,  tr, vl, h, l, pred, plotcheck = False)\n",
    "    data_tt = torch.from_numpy(train_data)\n",
    "    train_data = data_tt.to(torch.float).to(device)\n",
    "    data_tt = torch.from_numpy(test_data)\n",
    "    test_data = data_tt.to(torch.float).to(device)\n",
    "    data_tt = torch.from_numpy(val_data)\n",
    "    validation_data = data_tt.to(torch.float).to(device)\n",
    "\n",
    "    condition_size = l\n",
    "    target_size = pred\n",
    "    ref_mean = torch.mean(train_data[0:batch_size,:])\n",
    "    ref_std = torch.std(train_data[0:batch_size,:])\n",
    "\n",
    "    gen = LSTM(noise_dim = 0,cond_dim=condition_size, hidden_dim=hid_g,output_dim=pred,mean =ref_mean,std=ref_std)\n",
    "    gen.to(device)\n",
    "    criterion = False\n",
    "\n",
    "    PnL_test = [False] * 6\n",
    "    gen_opt = torch.optim.RMSprop(gen.parameters(), lr=lrg)\n",
    "\n",
    "    gen, gen_opt,  alpha, beta, gamma, delta = GradientCheckLSTM(ticker, gen, gen_opt, ngrad, train_data,batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "\n",
    "    f_name = modelsloc + ticker + \"-LSTM-\"+str(n_epochs)+\"-epochs-\"+str(lrd)+\"-lrd-\"+str(lrg)+\"-lrg\"\n",
    "    f_name1 = ticker + \"-LSTM-\"+str(n_epochs)+\"-epochs-\"+str(lrd)+\"-lrd-\"+str(lrg)+\"-lrg\"\n",
    "\n",
    "    print(\"PnL\")\n",
    "    losstype = \"PnL\"\n",
    "    genPnL,  gen_optPnL = TrainLoopnLSTMPnL(gen,  gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnL.state_dict()}, f_name + \"PnL_lstm_checkpoint.pth\")\n",
    "    df_temp, PnL_test[0], PnL_even, PnL_odd = Evaluation2LSTM(ticker,freq,genPnL,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL LSTM\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "\n",
    "    ntest = test_data.shape[0]\n",
    "    pd.DataFrame(PnL_test[0]).to_csv(loc+\"PnLs/\"+ticker+\"-LSTM-\"+losstype+\".csv\")\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.title(\"Cummulative PnL \"+ticker)\n",
    "    plt.grid(visible=True)\n",
    "    plt.xlabel(\"date\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"bpts\")\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[0]), label = \"PnL\")\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.title(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.grid(visible=True)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.ylabel(\"bpts\")\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.title(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(visible=True)\n",
    "\n",
    "        plt.ylabel(\"bpts\")\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.title(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.grid(visible=True)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"bpts\")\n",
    "\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.title(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.xlabel(\"date\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(visible=True)\n",
    "        plt.ylabel(\"bpts\")\n",
    "\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = \"PnL\")\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL STD\")\n",
    "    losstype = \"PnL STD\"\n",
    "    genPnLMSESTD, gen_optPnLMSESTD = TrainLoopnLSTMPnLSTD(gen,  gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLMSESTD.state_dict()}, f_name + \"PnLMSESTD_lstm_checkpoint.pth\")\n",
    "    df_temp, PnL_test[1], PnL_even, PnL_odd = Evaluation2LSTM(ticker,freq,genPnLMSESTD,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL STD LSTM\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    pd.DataFrame(PnL_test[1]).to_csv(loc+\"PnLs/\"+ticker+\"-LSTM-\"+losstype+\".csv\")\n",
    "\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[1]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL SR\")\n",
    "    losstype = \"PnL SR\"\n",
    "    genPnLMSESR, gen_optPnLMSESR = TrainLoopnLSTMPnLSR(gen, gen_opt,  criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLMSESR.state_dict()}, f_name + \"PnLMSESR_lstm_checkpoint.pth\")\n",
    "    df_temp, PnL_test[2], PnL_even, PnL_odd = Evaluation2LSTM(ticker,freq,genPnLMSESR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL SR LSTM\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    pd.DataFrame(PnL_test[2]).to_csv(loc+\"PnLs/\"+ticker+\"-LSTM-\"+losstype+\".csv\")\n",
    "\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[1]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    print(\"STD\")\n",
    "    losstype = \"STD\"\n",
    "    genPnLSR, gen_optPnLMSESR = TrainLoopnLSTMSTD(gen, gen_opt,  criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"STD_lstm_checkpoint.pth\")\n",
    "    df_temp, PnL_test[3], PnL_even, PnL_odd = Evaluation2LSTM(ticker,freq,genPnLSR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"STD LSTM\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    pd.DataFrame(PnL_test[3]).to_csv(loc+\"PnLs/\"+ticker+\"-LSTM-\"+losstype+\".csv\")\n",
    "\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[1]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    print(\"SR\")\n",
    "    losstype = \"SR\"\n",
    "    genSR, gen_optSR = TrainLoopnLSTMSR(gen,  gen_opt , criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"SR_lstm_checkpoint.pth\")\n",
    "    df_temp, PnL_test[4], PnL_even, PnL_odd = Evaluation2LSTM(ticker,freq,genSR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"SR LSTM\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    pd.DataFrame(PnL_test[4]).to_csv(loc+\"PnLs/\"+ticker+\"-LSTM-\"+losstype+\".csv\")\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[1]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    print(\"MSE\")\n",
    "    losstype = \"MSE\"\n",
    "    genMSE, gen_optMSE = TrainLoopnLSTM(gen, gen_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data, batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genMSE.state_dict()}, f_name + \"MSE_lstm_checkpoint.pth\")\n",
    "    df_temp, PnL_test[5], PnL_even, PnL_odd = Evaluation2LSTM(ticker,freq,genMSE,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"MSE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    pd.DataFrame(PnL_test[5]).to_csv(loc+\"PnLs/\"+ticker+\"-LSTM-\"+losstype+\".csv\")\n",
    "    plt.figure(\"Cummulative PnL \"+ticker)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_test[1]), label = losstype)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(plotsloc+\"LSTM-\"+ticker+\"-cummulativePnL.png\")\n",
    "\n",
    "    if (test_data.shape[0] % 2 == 0):\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(plotsloc+\"LSTM-\"+ticker+\"-intradayPnL.png\")\n",
    "\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(plotsloc+\"LSTM-\"+ticker+\"-overnight.png\")\n",
    "    else:\n",
    "        plt.figure(\"Overnight cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_odd), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(plotsloc+\"LSTM-\"+ticker+\"-overnight.png\")\n",
    "\n",
    "        plt.figure(\"Intraday cummulative PnL \"+ticker)\n",
    "        plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnL_even), label = losstype)\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(plotsloc+\"LSTM-\"+ticker+\"-intradayPnL.png\")\n",
    "\n",
    "    corrm = np.corrcoef(PnL_test)\n",
    "\n",
    "    return results_df, corrm\n",
    "\n",
    "def FinGAN_universal(tickers1, other,loc,modelsloc,plotsloc,dataloc, etflistloc,  vl_later = True, lrg = 0.0001, lrd = 0.0001, n_epochs = 500, ngrad = 100, h = 1, l = 10, pred = 1, ngpu = 1, tanh_coeff = 100, tr = 0.8, vl = 0.1, z_dim = 32, hid_d = 64, hid_g = 8, checkpoint_epoch = 20, batch_size = 100, diter = 1, plot = False, freq = 2):\n",
    "    \"\"\"\n",
    "    FinGAN loss combos in the universal setting\n",
    "    \"\"\"\n",
    "    #initialise the networks first:\n",
    "    datastart = {'lrd':[],'lrg':[],'epochs':[],'SR_val':[]}\n",
    "    results_df = pd.DataFrame(data=datastart)\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "    ticker = tickers1[0]\n",
    "    train_data,val_data,test_data, dates_dt = split_train_val_test(ticker, dataloc, etflistloc,  tr, vl, h, l, pred, plot)\n",
    "    ntr = train_data.shape[0]\n",
    "    nvl = val_data.shape[0]\n",
    "    ntest = test_data.shape[0]\n",
    "    n_tickers1 = len(tickers1)\n",
    "    n_tickers = len(tickers1) + len(other)\n",
    "    train_data = np.zeros((ntr * n_tickers1, l + pred))\n",
    "    validation_data = [False] * n_tickers\n",
    "    test_data = [False] * n_tickers\n",
    "    for i in range(n_tickers1):\n",
    "        ticker = tickers1[i]\n",
    "        if ticker[0] == \"X\":\n",
    "            train,val,test, _ = split_train_val_testraw(ticker, dataloc,  tr, vl, h, l, pred, plot)\n",
    "        else:\n",
    "            train,val,test, _ = split_train_val_test(ticker, dataloc, etflistloc,  tr, vl, h, l, pred, plot)\n",
    "        data_tt = torch.from_numpy(test)\n",
    "        test_data[i] = data_tt.to(torch.float).to(device)\n",
    "        train_data[i*ntr:(i+1)*ntr] = train\n",
    "        data_tt = torch.from_numpy(val)\n",
    "        validation_data[i] = data_tt.to(torch.float).to(device)\n",
    "    data_tt = torch.from_numpy(train_data)\n",
    "    train_data = data_tt.to(torch.float).to(device)\n",
    "    for i in range(len(other)):\n",
    "        ticker = tickers1[i]\n",
    "        _,val,test, _ = split_train_val_test(ticker, dataloc, etflistloc,  tr, vl, h, l, pred, plot)\n",
    "        data_tt = torch.from_numpy(test)\n",
    "        test_data[i + n_tickers1] = data_tt.to(torch.float).to(device)\n",
    "        data_tt = torch.from_numpy(val)\n",
    "        validation_data[i + n_tickers1] = data_tt.to(torch.float).to(device)\n",
    "\n",
    "    tickers = np.concatenate((tickers1,other))\n",
    "    condition_size = l\n",
    "    target_size = pred\n",
    "    ref_mean = torch.mean(train_data[0:batch_size,:])\n",
    "    ref_std = torch.std(train_data[0:batch_size,:])\n",
    "    discriminator_indim = condition_size+target_size\n",
    "\n",
    "    gen = Generator(noise_dim=z_dim,cond_dim=condition_size, hidden_dim=hid_g,output_dim=pred,mean =ref_mean,std=ref_std)\n",
    "    gen.to(device)\n",
    "\n",
    "    disc = Discriminator(in_dim=discriminator_indim, hidden_dim=hid_d,mean=ref_mean,std=ref_std)\n",
    "    disc.to(device)\n",
    "\n",
    "    gen_opt = torch.optim.RMSprop(gen.parameters(), lr=lrg)\n",
    "    disc_opt = torch.optim.RMSprop(disc.parameters(), lr=lrd)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta = GradientCheck(ticker, gen, disc, gen_opt, disc_opt, criterion, ngrad, train_data,batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "\n",
    "    f_name = modelsloc +  \"vuniversal-\"+str(n_epochs)+\"-epochs-\"+str(lrd)+\"-lrd-\"+str(lrg)+\"-lrg\"\n",
    "    f_name1 = ticker + \"-universal-\"+str(n_epochs)+\"-epochs-\"+str(lrd)+\"-lrd-\"+str(lrg)+\"-lrg\"\n",
    "\n",
    "    PnLs_test = [False] * 10\n",
    "    PnLs_val = [False] * 10\n",
    "    means_test = [False] * 10\n",
    "    means_val = [False] * 10\n",
    "    print(\"PnL\")\n",
    "    losstype = \"PnL\"\n",
    "    genPnL, discPnL, gen_optPnL, disc_optPnL = TrainLoopMainPnLnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnL.state_dict()}, f_name + \"PnL_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[0], PnLs_val[0], means_test[0], means_val[0] = Evaluation3(tickers,freq,genPnL,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.title(\"Portfolio cummulative PnL \" )\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[0]),label=losstype)\n",
    "    plt.grid(visible=True)\n",
    "    plt.ylabel(\"bpts\")\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "    print(\"PnL MSE\")\n",
    "    genPnLMSE, discPnLMSE, gen_optPnLMSE, disc_optPnLMSE = TrainLoopMainPnLMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLMSE.state_dict()}, f_name + \"PnLMSE_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[1], PnLs_val[1], means_test[1], means_val[1] = Evaluation3(tickers,freq,genPnLMSE,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL MSE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"PnL MSE\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[1]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL MSE STD\")\n",
    "    genPnLMSESTD, discPnLMSESTD, gen_optPnLMSESTD, disc_optPnLMSESTD = TrainLoopMainPnLMSESTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLMSESTD.state_dict()}, f_name + \"PnLMSESTD_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[2], PnLs_val[2], means_test[2], means_val[2]= Evaluation3(tickers,freq,genPnLMSESTD,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL MSE STD\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"PnL MSE STD\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[2]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL MSE SR\")\n",
    "    genPnLMSESR, discPnLMSESR, gen_optPnLMSESR, disc_optPnLMSESR = TrainLoopMainPnLMSESRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLMSESR.state_dict()}, f_name + \"PnLMSESR_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[3], PnLs_val[3], means_test[3], means_val[3] = Evaluation3(tickers,freq,genPnLMSESR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL MSE SR\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"PnL MSE SR\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[3]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL SR\")\n",
    "    genPnLSR, discPnLSR, gen_optPnLSR, disc_optPnLSR = TrainLoopMainPnLSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"PnLSR_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[4], PnLs_val[4], means_test[4], means_val[4] = Evaluation3(tickers,freq,genPnLSR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL SR\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"PnL SR\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[4]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"PnL STD\")\n",
    "    genPnLSTD, discPnLSTD, gen_optPnLSTD, disc_optPnLSTD = TrainLoopMainPnLSTDnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"PnLSTD_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[5], PnLs_val[5], means_test[5], means_val[5] = Evaluation3(tickers,freq,genPnLSTD,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"PnL STD\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"PnL STD\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[5]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"SR\")\n",
    "    genSR, discSR, gen_optSR, disc_optSR = TrainLoopMainSRnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genPnLSR.state_dict()}, f_name + \"SR_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[6], PnLs_val[6], means_test[6], means_val[6] = Evaluation3(tickers,freq,genSR,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"SR\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"SR\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[6]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"SR MSE\")\n",
    "    genSRMSE, discSRMSE, gen_optSRMSE, disc_optSRMSE = TrainLoopMainSRMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genSRMSE.state_dict()}, f_name + \"SRMSE_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[7], PnLs_val[7], means_test[7], means_val[7] = Evaluation3(tickers,freq,genSRMSE,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"SR MSE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"SR MSE\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[7]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"MSE\")\n",
    "    genMSE, discMSE, gen_optMSE, disc_optMSE = TrainLoopMainMSEnv(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genMSE.state_dict()}, f_name + \"MSE_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[8], PnLs_val[8], means_test[8], means_val[8] = Evaluation3(tickers,freq,genMSE,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"MSE\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"MSE\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[8]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    print(\"ForGAN\")\n",
    "    genFG, discFG, gen_optFG, disc_optFG = TrainLoopForGAN(gen, disc, gen_opt, disc_opt, criterion, alpha, beta, gamma, delta, n_epochs, checkpoint_epoch, train_data, validation_data[0], batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "    torch.save({'g_state_dict': genFG.state_dict()}, f_name + \"ForGAN_generator_checkpoint.pth\")\n",
    "    df_temp, PnLs_test[9], PnLs_val[9], means_test[9], means_val[9] = Evaluation3(tickers,freq,genFG,test_data,validation_data,h,l,pred,hid_d,hid_g, z_dim, lrg, lrd, n_epochs, \"ForGAN\", 0, device, plotsloc, f_name1)\n",
    "    results_df = pd.concat([results_df,df_temp], ignore_index=True)\n",
    "    losstype = \"BCE\"\n",
    "    plt.figure(\" portfolio cumPnL- \"+ f_name)\n",
    "    plt.plot(dates_dt[-int(ntest/2):], np.cumsum(PnLs_test[9]),label=losstype)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(plotsloc+\"UniversalPnLCumm.png\")\n",
    "\n",
    "    return results_df, PnLs_test, PnLs_val, means_test, means_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy.random as rnd\n",
    "\n",
    "def FinGAN_combos(ticker,loc,modelsloc,plotsloc,dataloc, etflistloc,  vl_later = True, lrg = 0.0001, lrd = 0.0001, n_epochs = 500, ngrad = 100, h = 1, l = 10, pred = 1, ngpu = 1, tanh_coeff = 100, tr = 0.8, vl = 0.1, z_dim = 32, hid_d = 64, hid_g = 8, checkpoint_epoch = 20, batch_size = 100, diter = 1, plot = False, freq = 2):\n",
    "    \"\"\"\n",
    "    FinGAN: looking at all combinations, performance on both validation and test set for all\n",
    "    \"\"\"\n",
    "    #initialise the networks first:\n",
    "    datastart = {'lrd':[],'lrg':[],'epochs':[],'SR_val':[]}\n",
    "    results_df = pd.DataFrame(data=datastart)\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "    if ticker[0] == 'X':\n",
    "        train_data,val_data,test_data, dates_dt = split_train_val_testraw(ticker, dataloc, tr, vl, h, l, pred, plotcheck = False)\n",
    "    else:\n",
    "        train_data,val_data,test_data, dates_dt = split_train_val_test(ticker, dataloc, etflistloc,  tr, vl, h, l, pred, plotcheck = False)\n",
    "    data_tt = torch.from_numpy(train_data)\n",
    "    train_data = data_tt.to(torch.float).to(device)\n",
    "    data_tt = torch.from_numpy(test_data)\n",
    "    test_data = data_tt.to(torch.float).to(device)\n",
    "    data_tt = torch.from_numpy(val_data)\n",
    "    validation_data = data_tt.to(torch.float).to(device)\n",
    "    ntest = test_data.shape[0]\n",
    "    condition_size = l\n",
    "    target_size = pred\n",
    "    ref_mean = torch.mean(train_data[0:batch_size,:])\n",
    "    ref_std = torch.std(train_data[0:batch_size,:])\n",
    "    discriminator_indim = condition_size+target_size\n",
    "\n",
    "    gen = Generator(noise_dim=z_dim,cond_dim=condition_size, hidden_dim=hid_g,output_dim=pred,mean =ref_mean,std=ref_std)\n",
    "    gen.to(device)\n",
    "\n",
    "    disc = Discriminator(in_dim=discriminator_indim, hidden_dim=hid_d,mean=ref_mean,std=ref_std)\n",
    "    disc.to(device)\n",
    "\n",
    "    gen_opt = torch.optim.RMSprop(gen.parameters(), lr=lrg)\n",
    "    disc_opt = torch.optim.RMSprop(disc.parameters(), lr=lrd)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta = GradientCheck(ticker, gen, disc, gen_opt, disc_opt, criterion, ngrad, train_data,batch_size,hid_d, hid_g, z_dim, lrd, lrg, h, l, pred, diter, tanh_coeff, device, plot)\n",
    "\n",
    "    return  gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "Processing Ticker: TCS\n",
      "******************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4237/4237 [00:00<00:00, 2773293.70it/s]\n",
      "100%|██████████| 520/520 [00:00<00:00, 2640481.94it/s]\n",
      "100%|██████████| 522/522 [00:00<00:00, 1771380.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Gradient Check for Ticker: TCS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Ticker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m******************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m df_temp, corrs[tickern] \u001b[38;5;241m=\u001b[39m \u001b[43mFinGAN_combos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodelsloc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplotsloc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43metflistloc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvl_later\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlrg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlrd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtanh_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhid_d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhid_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mditer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mditer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results_df, df_temp], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    100\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(resultsloc \u001b[38;5;241m+\u001b[39m resultsname)\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mFinGAN_combos\u001b[0;34m(ticker, loc, modelsloc, plotsloc, dataloc, etflistloc, vl_later, lrg, lrd, n_epochs, ngrad, h, l, pred, ngpu, tanh_coeff, tr, vl, z_dim, hid_d, hid_g, checkpoint_epoch, batch_size, diter, plot, freq)\u001b[0m\n\u001b[1;32m     41\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     42\u001b[0m criterion \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta \u001b[38;5;241m=\u001b[39m \u001b[43mGradientCheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhid_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mditer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtanh_coeff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta\n",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m, in \u001b[0;36mGradientCheck\u001b[0;34m(ticker, gen, disc, gen_opt, disc_opt, criterion, n_epochs, train_data, batch_size, hid_d, hid_g, z_dim, lr_d, lr_g, h, l, pred, diter, tanh_coeff, device, plot)\u001b[0m\n\u001b[1;32m     17\u001b[0m epoch_gradient_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m epoch_gradient_disc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_data):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Move inputs and targets to the specified device\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import FinGAN\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters and Configurations\n",
    "h = 1\n",
    "l = 10\n",
    "pred = 1\n",
    "\n",
    "dataloc = \"/home/harsh/Hackathons/TradeGAN/data/\"\n",
    "etflistloc = \"/home/harsh/Hackathons/TradeGAN/stocks-etfs-list.csv\"\n",
    "\n",
    "n_epochs = 100\n",
    "ngpu = 1\n",
    "\n",
    "loc = \"/home/harsh/Hackathons/TradeGAN/Fin-GAN/\"\n",
    "modelsloc = loc + \"TrainedModels/\"\n",
    "plotsloc = loc + \"Plots/\"\n",
    "resultsloc = loc + \"Results/\"\n",
    "\n",
    "# Model Parameters\n",
    "tanh_coeff = 100\n",
    "z_dim = 8\n",
    "hid_d = 8\n",
    "hid_g = 8\n",
    "\n",
    "# Checkpoint and Batch Settings\n",
    "checkpoint_epoch = 20\n",
    "batch_size = 100\n",
    "diter = 1\n",
    "\n",
    "# Learning Rate Exploration\n",
    "lrg_s = [0.0001]\n",
    "lrd_s = [0.0001]\n",
    "hid_d_s = [8]\n",
    "hid_g_s = [8]\n",
    "nres = len(lrg_s)\n",
    "\n",
    "# Data Split Ratios\n",
    "tr = 0.8\n",
    "vl = 0.1\n",
    "ngrad = 100\n",
    "vl_later = True\n",
    "\n",
    "# Plot Configuration\n",
    "plot = False\n",
    "\n",
    "# Initial Data Structure for Results\n",
    "datastart = {'lrd': [], 'lrg': [], 'epochs': [], 'SR_val': []}\n",
    "results_df = pd.DataFrame(data=datastart)\n",
    "\n",
    "# Tickers to Analyze\n",
    "tickers = ['TCS']\n",
    "corrs = [False] * len(tickers)\n",
    "\n",
    "# Results Filename\n",
    "resultsname = \"results.csv\"\n",
    "plt.rcParams['figure.figsize'] = [15.75, 9.385]\n",
    "\n",
    "for j in range(len(hid_d_s)):\n",
    "    for i in range(nres):\n",
    "        lrg = lrg_s[i]\n",
    "        lrd = lrd_s[i]\n",
    "\n",
    "        for tickern in range(len(tickers)):\n",
    "            ticker = tickers[tickern]\n",
    "            print(\"******************\")\n",
    "            print(f\"Processing Ticker: {ticker}\")\n",
    "            print(\"******************\")\n",
    "\n",
    "            df_temp, corrs[tickern] = FinGAN_combos(\n",
    "                ticker,\n",
    "                loc,\n",
    "                modelsloc,\n",
    "                plotsloc,\n",
    "                dataloc,\n",
    "                etflistloc,\n",
    "                vl_later,\n",
    "                lrg,\n",
    "                lrd,\n",
    "                n_epochs,\n",
    "                ngrad,\n",
    "                h,\n",
    "                l,\n",
    "                pred,\n",
    "                ngpu,\n",
    "                tanh_coeff,\n",
    "                tr,\n",
    "                vl,\n",
    "                z_dim,\n",
    "                hid_d,\n",
    "                hid_g,\n",
    "                checkpoint_epoch,\n",
    "                batch_size=batch_size,\n",
    "                diter=diter,\n",
    "                plot=plot\n",
    "            )\n",
    "\n",
    "            results_df = pd.concat([results_df, df_temp], ignore_index=True)\n",
    "            results_df.to_csv(resultsloc + resultsname)\n",
    "\n",
    "            print(f\"Completed Processing (FinGAN Combos) for Ticker: {ticker}\")\n",
    "\n",
    "            print(\"******************\")\n",
    "            # print(f\"Processing Ticker (LSTM Combos): {ticker}\")\n",
    "            # print(\"******************\")\n",
    "\n",
    "            # e = FinGAN.LSTM_combos(\n",
    "            #     ticker,\n",
    "            #     loc,\n",
    "            #     modelsloc,\n",
    "            #     plotsloc,\n",
    "            #     dataloc,\n",
    "            #     etflistloc,\n",
    "            #     vl_later=True,\n",
    "            #     lrg=0.0001,\n",
    "            #     lrd=0.0001,\n",
    "            #     n_epochs=500,\n",
    "            #     ngrad=100,\n",
    "            #     h=1,\n",
    "            #     l=10,\n",
    "            #     pred=1,\n",
    "            #     ngpu=1,\n",
    "            #     tanh_coeff=100,\n",
    "            #     tr=0.8,\n",
    "            #     vl=0.1,\n",
    "            #     z_dim=32,\n",
    "            #     hid_d=64,\n",
    "            #     hid_g=8,\n",
    "            #     checkpoint_epoch=20,\n",
    "            #     batch_size=100,\n",
    "            #     diter=1,\n",
    "            #     plot=False,\n",
    "            #     freq=2\n",
    "            # )\n",
    "\n",
    "            # print(f\"Completed Processing (LSTM Combos) for Ticker: {ticker}\")\n",
    "            print(\"*************\")\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4237/4237 [00:00<00:00, 2360061.89it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 520/520 [00:00<00:00, 2146691.02it/s]\n",
      "100%|██████████| 522/522 [00:00<00:00, 1413445.25it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mean(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 137\u001b[0m\n\u001b[1;32m    122\u001b[0m training_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    134\u001b[0m }\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Execute Gradient Check\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[43mgradient_check_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metflistloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 53\u001b[0m, in \u001b[0;36mgradient_check_plot\u001b[0;34m(ticker, dataloc, etflistloc, gen_params, disc_params, training_params, plot)\u001b[0m\n\u001b[1;32m     45\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize Generator and Discriminator\u001b[39;00m\n\u001b[1;32m     48\u001b[0m gen \u001b[38;5;241m=\u001b[39m Generator(\n\u001b[1;32m     49\u001b[0m     noise_dim\u001b[38;5;241m=\u001b[39mgen_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_dim\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     50\u001b[0m     cond_dim\u001b[38;5;241m=\u001b[39mtraining_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     51\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mgen_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhid_g\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     52\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39mtraining_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m---> 53\u001b[0m     mean\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     54\u001b[0m     std\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstd(train_data[\u001b[38;5;241m0\u001b[39m:training_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], :])\n\u001b[1;32m     55\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m disc \u001b[38;5;241m=\u001b[39m Discriminator(\n\u001b[1;32m     58\u001b[0m     in_dim\u001b[38;5;241m=\u001b[39mtraining_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m training_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     59\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mdisc_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhid_d\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     60\u001b[0m     mean\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(train_data[\u001b[38;5;241m0\u001b[39m:training_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], :]),\n\u001b[1;32m     61\u001b[0m     std\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstd(train_data[\u001b[38;5;241m0\u001b[39m:training_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], :])\n\u001b[1;32m     62\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Optimizers and Loss Function\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: mean(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to Perform Gradient Check and Plot Results\n",
    "def gradient_check_plot(ticker, dataloc, etflistloc, gen_params, disc_params, training_params, plot=True):\n",
    "    \"\"\"\n",
    "    Perform a gradient check for a given ticker and plot the results.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        The stock ticker for which to perform the gradient check.\n",
    "    dataloc : str\n",
    "        Location of the stock and ETF data files.\n",
    "    etflistloc : str\n",
    "        Location of the ETF-stock relationship file.\n",
    "    gen_params : dict\n",
    "        Parameters for the Generator (e.g., dimensions, mean, std).\n",
    "    disc_params : dict\n",
    "        Parameters for the Discriminator (e.g., dimensions, mean, std).\n",
    "    training_params : dict\n",
    "        Training hyperparameters like learning rates, epochs, etc.\n",
    "    plot : bool\n",
    "        Whether to plot the gradient check results.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Split Data\n",
    "    train_data, val_data, test_data, _ = split_train_val_test(\n",
    "        stock=ticker,\n",
    "        dataloc=dataloc,\n",
    "        etflistloc=etflistloc,\n",
    "        tr=training_params['tr'],\n",
    "        vl=training_params['vl'],\n",
    "        h=training_params['h'],\n",
    "        l=training_params['l'],\n",
    "        pred=training_params['pred'],\n",
    "        plotcheck=False\n",
    "    )\n",
    "\n",
    "    # Device Configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    gen = Generator(\n",
    "        noise_dim=gen_params['z_dim'],\n",
    "        cond_dim=training_params['l'],\n",
    "        hidden_dim=gen_params['hid_g'],\n",
    "        output_dim=training_params['pred'],\n",
    "        mean=torch.mean(train_data[0:training_params['batch_size'], :]),\n",
    "        std=torch.std(train_data[0:training_params['batch_size'], :])\n",
    "    ).to(device)\n",
    "\n",
    "    disc = Discriminator(\n",
    "        in_dim=training_params['l'] + training_params['pred'],\n",
    "        hidden_dim=disc_params['hid_d'],\n",
    "        mean=torch.mean(train_data[0:training_params['batch_size'], :]),\n",
    "        std=torch.std(train_data[0:training_params['batch_size'], :])\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizers and Loss Function\n",
    "    gen_opt = torch.optim.RMSprop(gen.parameters(), lr=training_params['lr_g'])\n",
    "    disc_opt = torch.optim.RMSprop(disc.parameters(), lr=training_params['lr_d'])\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # Perform Gradient Check\n",
    "    gen, disc, gen_opt, disc_opt, alpha, beta, gamma, delta = GradientCheck(\n",
    "        ticker=ticker,\n",
    "        gen=gen,\n",
    "        disc=disc,\n",
    "        gen_opt=gen_opt,\n",
    "        disc_opt=disc_opt,\n",
    "        criterion=criterion,\n",
    "        n_epochs=training_params['n_epochs'],\n",
    "        train_data=torch.from_numpy(train_data).float().to(device),\n",
    "        batch_size=training_params['batch_size'],\n",
    "        hid_d=disc_params['hid_d'],\n",
    "        hid_g=gen_params['hid_g'],\n",
    "        z_dim=gen_params['z_dim'],\n",
    "        lr_d=training_params['lr_d'],\n",
    "        lr_g=training_params['lr_g'],\n",
    "        h=training_params['h'],\n",
    "        l=training_params['l'],\n",
    "        pred=training_params['pred'],\n",
    "        diter=training_params['diter'],\n",
    "        tanh_coeff=training_params['tanh_coeff'],\n",
    "        device=device,\n",
    "        plot=plot\n",
    "    )\n",
    "\n",
    "    # Print Hyperparameters and Results\n",
    "    print(\"\\nGradient Check Completed for Ticker:\", ticker)\n",
    "    print(\"Results:\")\n",
    "    print(f\"Alpha (BCE/PnL): {alpha:.4f}\")\n",
    "    print(f\"Beta (BCE/MSE): {beta:.4f}\")\n",
    "    print(f\"Gamma (BCE/SR): {gamma:.4f}\")\n",
    "    print(f\"Delta (BCE/STD): {delta:.4f}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Data Locations\n",
    "    dataloc = \"./data/\"  # Replace with your data directory\n",
    "    etflistloc = \"stocks-etfs-list.csv\"  # Replace with your ETF list CSV\n",
    "\n",
    "    # Ticker to Analyze\n",
    "    ticker = \"TCS\"  # Replace with your stock ticker\n",
    "\n",
    "    # Generator and Discriminator Parameters\n",
    "    gen_params = {\n",
    "        \"z_dim\": 8,\n",
    "        \"hid_g\": 8\n",
    "    }\n",
    "\n",
    "    disc_params = {\n",
    "        \"hid_d\": 8\n",
    "    }\n",
    "\n",
    "    # Training Parameters\n",
    "    training_params = {\n",
    "        \"tr\": 0.8,\n",
    "        \"vl\": 0.1,\n",
    "        \"h\": 1,\n",
    "        \"l\": 10,\n",
    "        \"pred\": 1,\n",
    "        \"batch_size\": 100,\n",
    "        \"n_epochs\": 100,\n",
    "        \"lr_g\": 0.0001,\n",
    "        \"lr_d\": 0.0001,\n",
    "        \"diter\": 1,\n",
    "        \"tanh_coeff\": 100\n",
    "    }\n",
    "\n",
    "    # Execute Gradient Check\n",
    "    gradient_check_plot(ticker, dataloc, etflistloc, gen_params, disc_params, training_params, plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
